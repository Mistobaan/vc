{"id":"vc-1","title":"Activity Feed and Event Streaming","description":"Implement comprehensive event journaling and real-time activity feed for observability. Without this, debugging AI agents is nearly impossible. Must-have before dogfooding.","design":"Event-based system that logs all executor actions, AI assessments, issue state changes, and agent interactions. Provides 'vc executor tail' command for real-time monitoring.","acceptance_criteria":"- Event journal table in database\n- Activity feed CLI command working\n- Real-time tail functionality\n- Integration with issue processor\n- Events logged for: issue claims, AI assessments, executions, quality gates, issue closures","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-13T21:05:19.448795-07:00","updated_at":"2025-10-15T19:48:22.305848-07:00","dependencies":[{"issue_id":"vc-1","depends_on_id":"vc-9","type":"blocks","created_at":"2025-10-13T21:05:19.449797-07:00","created_by":"import"}]}
{"id":"vc-10","title":"Verify existing Beads fields and add discovered-from dependency type","description":"Quick verification that design, acceptance_criteria, and notes fields work correctly. Add the 'discovered-from' dependency type constant to support tracking work discovered during execution.","design":"1. Write test to verify design/acceptance_criteria/notes fields can be set and retrieved. 2. Add DepDiscoveredFrom constant to internal/types/types.go. 3. Update DependencyType.IsValid() to include new type. 4. Test creating dependencies with discovered-from type.","acceptance_criteria":"- Test confirms design, acceptance_criteria, notes fields work correctly\\n- DepDiscoveredFrom constant added to types\\n- IsValid() function updated\\n- Can create and query dependencies with type 'discovered-from'\\n- Documentation updated","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-13T21:21:48.323451-07:00","updated_at":"2025-10-15T11:52:52.173133-07:00","closed_at":"2025-10-13T23:14:01.180328-07:00","dependencies":[{"issue_id":"vc-10","depends_on_id":"vc-5","type":"parent-child","created_at":"2025-10-13T21:22:52.23331-07:00","created_by":"stevey"}]}
{"id":"vc-100","title":"Implement ContextGatherer with all context sources","description":"Implement context gathering from all sources: issue tree, sandbox, history, git state","design":"\nCreate internal/executor/gatherer.go:\n\n```go\ntype contextGatherer struct {\n    store      storage.Storage\n    sandboxMgr sandbox.Manager\n}\n\nfunc NewContextGatherer(store storage.Storage, sandboxMgr sandbox.Manager) ContextGatherer\n\nfunc (g *contextGatherer) GatherContext(ctx context.Context, issue *types.Issue, sb *sandbox.Sandbox) (*PromptContext, error) {\n    pc := \u0026PromptContext{Issue: issue}\n    \n    // 1. Get parent mission if this is a child task\n    pc.ParentMission, _ = g.GetParentMission(ctx, issue)\n    \n    // 2. Get related issues (blockers, dependents, siblings)\n    pc.RelatedIssues, _ = g.GetRelatedIssues(ctx, issue)\n    \n    // 3. Get previous execution attempts\n    pc.PreviousAttempts, _ = g.GetPreviousAttempts(ctx, issue.ID)\n    \n    // 4. Get quality gate status if any\n    pc.QualityGateStatus = g.getQualityGateStatus(ctx, issue)\n    \n    // 5. Get sandbox context if available\n    if sb != nil {\n        pc.Sandbox, _ = g.sandboxMgr.InspectState(ctx, sb)\n        pc.GitState = g.getGitState(ctx, sb)\n    }\n    \n    // 6. Analyze resume state\n    if len(pc.PreviousAttempts) \u003e 0 \u0026\u0026 sb != nil {\n        pc.ResumeHint, _ = g.AnalyzeResumeState(ctx, sb, pc.PreviousAttempts)\n    }\n    \n    return pc, nil\n}\n\nfunc (g *contextGatherer) GetParentMission(ctx context.Context, issue *types.Issue) (*types.Issue, error) {\n    // Find parent via parent-child dependency\n    deps, err := g.store.GetDependencies(ctx, issue.ID)\n    for _, dep := range deps {\n        // Check if this is a parent relationship\n        depRecords, _ := g.store.GetDependencyRecords(ctx, issue.ID)\n        for _, record := range depRecords {\n            if record.DependsOnID == dep.ID \u0026\u0026 record.Type == types.DepParentChild {\n                return dep, nil\n            }\n        }\n    }\n    return nil, nil\n}\n\nfunc (g *contextGatherer) GetRelatedIssues(ctx context.Context, issue *types.Issue) (*RelatedIssues, error) {\n    ri := \u0026RelatedIssues{}\n    \n    // Get blockers\n    ri.Blockers, _ = g.store.GetDependencies(ctx, issue.ID)\n    \n    // Get dependents\n    ri.Dependents, _ = g.store.GetDependents(ctx, issue.ID)\n    \n    // Get siblings (other children of same parent)\n    if parent, _ := g.GetParentMission(ctx, issue); parent != nil {\n        allChildren, _ := g.store.GetDependents(ctx, parent.ID)\n        for _, child := range allChildren {\n            if child.ID != issue.ID {\n                ri.Siblings = append(ri.Siblings, child)\n            }\n        }\n    }\n    \n    return ri, nil\n}\n\nfunc (g *contextGatherer) AnalyzeResumeState(ctx context.Context, sb *sandbox.Sandbox, attempts []*ExecutionAttempt) (string, error) {\n    // Analyze what was done and what remains\n    lastAttempt := attempts[len(attempts)-1]\n    \n    var hint strings.Builder\n    hint.WriteString(fmt.Sprintf(\"Previous attempt #%d \", lastAttempt.AttemptNumber))\n    if lastAttempt.Success {\n        hint.WriteString(\"succeeded but may have punted work. \")\n    } else {\n        hint.WriteString(fmt.Sprintf(\"failed with exit code %d. \", lastAttempt.ExitCode))\n    }\n    \n    // Add git state\n    if sbCtx, _ := g.sandboxMgr.InspectState(ctx, sb); sbCtx != nil {\n        if len(sbCtx.ModifiedFiles) \u003e 0 {\n            hint.WriteString(fmt.Sprintf(\"Modified files: %d. \", len(sbCtx.ModifiedFiles)))\n        }\n        if sbCtx.GitStatus != \"\" {\n            hint.WriteString(\"Uncommitted changes present. \")\n        }\n    }\n    \n    hint.WriteString(\"Please assess the current state and continue from where we left off.\")\n    return hint.String(), nil\n}\n```\n","acceptance_criteria":"\n- ContextGatherer implementation\n- GatherContext collects from all sources\n- GetParentMission finds parent via parent-child dependency\n- GetRelatedIssues finds blockers, dependents, siblings\n- GetPreviousAttempts retrieves execution history\n- AnalyzeResumeState generates helpful resume hint\n- Handles missing data gracefully (nil checks)\n- Unit tests for each method\n- Integration test with full context chain\n","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:24:52.946148-07:00","updated_at":"2025-10-15T19:48:11.18083-07:00","dependencies":[{"issue_id":"vc-100","depends_on_id":"vc-97","type":"parent-child","created_at":"2025-10-15T01:24:52.947678-07:00","created_by":"stevey"},{"issue_id":"vc-100","depends_on_id":"vc-98","type":"blocks","created_at":"2025-10-15T01:24:52.948251-07:00","created_by":"stevey"},{"issue_id":"vc-100","depends_on_id":"vc-99","type":"blocks","created_at":"2025-10-15T01:24:52.948605-07:00","created_by":"stevey"}]}
{"id":"vc-101","title":"Implement PromptBuilder with structured templates","description":"Build comprehensive prompts from PromptContext using structured templates","design":"\nCreate internal/executor/prompt.go:\n\n```go\ntype PromptBuilder struct {\n    template *template.Template\n}\n\nfunc NewPromptBuilder() *PromptBuilder\n\nfunc (pb *PromptBuilder) BuildPrompt(ctx *PromptContext) string {\n    // Use text/template to build structured prompt\n    var buf bytes.Buffer\n    pb.template.Execute(\u0026buf, ctx)\n    return buf.String()\n}\n```\n\n## Prompt Template Structure\n```markdown\n{{if .ParentMission -}}\n# MISSION CONTEXT\n\nYou are working on a subtask of a larger mission:\n\n**Mission**: {{.ParentMission.ID}} - {{.ParentMission.Title}}\n\n{{if .ParentMission.Description -}}\nMission Goal:\n{{.ParentMission.Description}}\n{{end}}\n{{end}}\n\n# YOUR TASK\n\n**Issue**: {{.Issue.ID}} - {{.Issue.Title}}\n\n{{if .Issue.Description -}}\n## Description\n{{.Issue.Description}}\n{{end}}\n\n{{if .Issue.Design -}}\n## Design\n{{.Issue.Design}}\n{{end}}\n\n{{if .Issue.AcceptanceCriteria -}}\n## Acceptance Criteria\n{{.Issue.AcceptanceCriteria}}\n{{end}}\n\n{{if .Sandbox -}}\n# ENVIRONMENT\n\nYou are working in an isolated sandbox:\n- **Path**: {{.Sandbox.Sandbox.Path}}\n- **Branch**: {{.Sandbox.Sandbox.GitBranch}}\n- **Database**: {{.Sandbox.Sandbox.BeadsDB}}\n\n{{if .Sandbox.ModifiedFiles -}}\nModified files ({{len .Sandbox.ModifiedFiles}}):\n{{range .Sandbox.ModifiedFiles -}}\n- {{.}}\n{{end}}\n{{end}}\n{{end}}\n\n{{if .RelatedIssues.Blockers -}}\n# BLOCKERS\n\nThis task depends on:\n{{range .RelatedIssues.Blockers -}}\n- {{.ID}}: {{.Title}} [{{.Status}}]\n{{end}}\n{{end}}\n\n{{if .RelatedIssues.Dependents -}}\n# DEPENDENT WORK\n\nThe following tasks are waiting for this:\n{{range .RelatedIssues.Dependents -}}\n- {{.ID}}: {{.Title}}\n{{end}}\n{{end}}\n\n{{if .PreviousAttempts -}}\n# PREVIOUS ATTEMPTS\n\nThis task has been attempted {{len .PreviousAttempts}} time(s) before:\n{{range .PreviousAttempts -}}\n## Attempt #{{.AttemptNumber}} ({{.StartedAt.Format \"2006-01-02 15:04\"}})\n- Result: {{if .Success}}✓ Success{{else}}✗ Failed{{end}}\n{{if .Summary -}}\n- Summary: {{.Summary}}\n{{end}}\n{{end}}\n\n{{if .ResumeHint -}}\n## Where We Left Off\n{{.ResumeHint}}\n{{end}}\n{{end}}\n\n{{if .QualityGateStatus -}}\n# QUALITY GATES\n{{if .QualityGateStatus.FailedGates -}}\n⚠️  The following quality gates failed:\n{{range .QualityGateStatus.FailedGates -}}\n- {{.Name}}: {{.Message}}\n{{end}}\n{{end}}\n{{end}}\n\n{{if .Issue.Notes -}}\n# NOTES\n{{.Issue.Notes}}\n{{end}}\n\n---\n\nPlease complete this task according to the acceptance criteria above.\n{{if .Sandbox -}}\nWork in the sandbox at: {{.Sandbox.Sandbox.Path}}\n{{end}}\n{{if .ResumeHint -}}\nContinue from where the previous attempt left off.\n{{end}}\n```\n","acceptance_criteria":"\n- PromptBuilder uses text/template\n- Template includes all context sections\n- Handles missing context gracefully (if checks)\n- Prompt is readable and well-structured\n- Parent mission context shown for child tasks\n- Previous attempts summarized\n- Resume hint highlighted\n- Quality gate failures shown\n- Sandbox environment details included\n- Unit tests with various context combinations\n- Sample prompts generated in tests for review\n","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:25:14.937941-07:00","updated_at":"2025-10-15T19:48:11.168227-07:00","dependencies":[{"issue_id":"vc-101","depends_on_id":"vc-97","type":"parent-child","created_at":"2025-10-15T01:25:14.938361-07:00","created_by":"stevey"},{"issue_id":"vc-101","depends_on_id":"vc-100","type":"blocks","created_at":"2025-10-15T01:25:14.938579-07:00","created_by":"stevey"}]}
{"id":"vc-102","title":"Replace buildPrompt with PromptBuilder in agent spawning","description":"Replace simple buildPrompt() with comprehensive PromptBuilder using gathered context","design":"\nModify internal/executor/agent.go:\n\n## Changes\n```go\n// Remove old buildPrompt function\n\n// Update buildClaudeCodeCommand and buildCodyCommand\nfunc buildClaudeCodeCommand(cfg AgentConfig, prompt string) *exec.Cmd {\n    // Now takes pre-built prompt instead of building it\n    args := []string{prompt}\n    return exec.Command(\"claude\", args...)\n}\n\nfunc buildCodyCommand(cfg AgentConfig, prompt string) *exec.Cmd {\n    // Now takes pre-built prompt\n    args := []string{\"chat\", \"--message\", prompt}\n    return exec.Command(\"cody\", args...)\n}\n```\n\n## Update SpawnAgent\n```go\nfunc SpawnAgent(ctx context.Context, cfg AgentConfig, prompt string) (*Agent, error) {\n    // Now takes pre-built prompt as parameter\n    // ... rest of implementation\n}\n```\n\n## Update Executor.executeIssue()\n```go\nfunc (e *Executor) executeIssue(ctx context.Context, issue *types.Issue) error {\n    // ... after sandbox creation ...\n    \n    // Gather context\n    gatherer := NewContextGatherer(e.store, e.sandboxMgr)\n    promptCtx, err := gatherer.GatherContext(ctx, issue, sandbox)\n    if err != nil {\n        return fmt.Errorf(\"failed to gather context: %w\", err)\n    }\n    \n    // Build prompt\n    builder := NewPromptBuilder()\n    prompt := builder.BuildPrompt(promptCtx)\n    \n    // Log prompt for debugging\n    if os.Getenv(\"VC_DEBUG_PROMPTS\") != \"\" {\n        fmt.Fprintf(os.Stderr, \"\\n=== AGENT PROMPT ===\\n%s\\n=== END PROMPT ===\\n\\n\", prompt)\n    }\n    \n    // Spawn agent with built prompt\n    agentCfg := AgentConfig{\n        Type:       AgentTypeClaudeCode,\n        WorkingDir: sandbox.Path,\n        Issue:      issue,\n        Sandbox:    sandbox,\n        // ... other fields ...\n    }\n    \n    agent, err := SpawnAgent(ctx, agentCfg, prompt)\n    // ... rest of execution ...\n}\n```\n","acceptance_criteria":"\n- buildPrompt() removed from agent.go\n- SpawnAgent takes pre-built prompt parameter\n- executeIssue() uses ContextGatherer and PromptBuilder\n- Prompt logged when VC_DEBUG_PROMPTS=1\n- Integration test: spawn agent with full context\n- Verify agent receives comprehensive prompt\n- Backward compatibility maintained (works without sandbox)\n","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:25:33.468746-07:00","updated_at":"2025-10-15T19:48:11.147244-07:00","dependencies":[{"issue_id":"vc-102","depends_on_id":"vc-97","type":"parent-child","created_at":"2025-10-15T01:25:33.47066-07:00","created_by":"stevey"},{"issue_id":"vc-102","depends_on_id":"vc-101","type":"blocks","created_at":"2025-10-15T01:25:33.471434-07:00","created_by":"stevey"}]}
{"id":"vc-103","title":"Structured Output Parsing and Event Extraction","description":"Parse agent stdout/stderr to extract structured events for activity feed, watchdog triggers, and progress monitoring. Track file modifications, test results, git operations, and behavioral anomalies.","design":"\n# Architecture\n\n## Core Components\n1. OutputParser: Parses raw output lines into structured events\n2. EventExtractor: Pattern matching for different event types\n3. EventStore: Persists events for activity feed\n4. WatchdogTrigger: Detects anomalous behavior\n\n## Event Types\n- FileModified: File changes detected\n- TestRun: Test execution results\n- GitOperation: Git commands executed\n- BuildOutput: Build/compile results\n- LintOutput: Linter findings\n- AgentProgress: Progress indicators\n- ErrorDetected: Errors or failures\n- WatchdogAlert: Behavioral anomalies\n\n## Event Structure\n```go\ntype AgentEvent struct {\n    ID        string\n    Type      EventType\n    Timestamp time.Time\n    IssueID   string\n    AgentID   string\n    Data      map[string]interface{}\n    Severity  EventSeverity\n}\n```\n\n## Detection Patterns\n- File modifications: \"Modified: \", \"Created: \", \"Deleted: \"\n- Test results: \"PASS\", \"FAIL\", \"test.*passed\", \"test.*failed\"\n- Git ops: \"git add\", \"git commit\", \"git rebase\"\n- Build output: \"error:\", \"warning:\", \"Build succeeded\"\n- Progress: \"Step X of Y\", \"[75%]\", \"Processing...\"\n\n## Watchdog Triggers\n- Infinite loops detected\n- Same file modified repeatedly (thrashing)\n- Tests passing then failing (regression)\n- Large file deletions\n- Git force operations\n- Excessive errors\n","acceptance_criteria":"\n- OutputParser extracts structured events from agent output\n- Supports all major event types\n- Events stored in database\n- Activity feed can query events\n- Watchdog can detect anomalies\n- Real-time event streaming during execution\n- Event severity classification\n- Integration with results processor\n- Unit tests for pattern matching\n- Integration tests with real agent output\n","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-15T01:25:52.538798-07:00","updated_at":"2025-10-15T19:47:16.995357-07:00"}
{"id":"vc-104","title":"Design AgentEvent types and EventStore interface","description":"Define event types, severity levels, and storage interface for agent events","design":"\nCreate internal/events/types.go:\n\n```go\ntype EventType string\nconst (\n    EventTypeFileModified  EventType = \"file_modified\"\n    EventTypeTestRun       EventType = \"test_run\"\n    EventTypeGitOperation  EventType = \"git_operation\"\n    EventTypeBuildOutput   EventType = \"build_output\"\n    EventTypeLintOutput    EventType = \"lint_output\"\n    EventTypeProgress      EventType = \"progress\"\n    EventTypeError         EventType = \"error\"\n    EventTypeWatchdog      EventType = \"watchdog_alert\"\n)\n\ntype EventSeverity string\nconst (\n    SeverityInfo    EventSeverity = \"info\"\n    SeverityWarning EventSeverity = \"warning\"\n    SeverityError   EventSeverity = \"error\"\n    SeverityCritical EventSeverity = \"critical\"\n)\n\ntype AgentEvent struct {\n    ID         string\n    Type       EventType\n    Timestamp  time.Time\n    IssueID    string\n    ExecutorID string\n    AgentID    string\n    Severity   EventSeverity\n    Message    string\n    Data       map[string]interface{} // JSON-serializable data\n    SourceLine int // Line number in agent output\n}\n\n// Specific event data structures\ntype FileModifiedData struct {\n    FilePath  string\n    Operation string // \"created\", \"modified\", \"deleted\"\n}\n\ntype TestRunData struct {\n    TestName string\n    Passed   bool\n    Duration time.Duration\n    Output   string\n}\n\ntype GitOperationData struct {\n    Command string\n    Args    []string\n    Success bool\n}\n\ntype EventStore interface {\n    StoreEvent(ctx context.Context, event *AgentEvent) error\n    GetEvents(ctx context.Context, filter EventFilter) ([]*AgentEvent, error)\n    GetEventsByIssue(ctx context.Context, issueID string) ([]*AgentEvent, error)\n    GetRecentEvents(ctx context.Context, limit int) ([]*AgentEvent, error)\n}\n\ntype EventFilter struct {\n    IssueID    string\n    Type       EventType\n    Severity   EventSeverity\n    AfterTime  time.Time\n    BeforeTime time.Time\n    Limit      int\n}\n```\n","acceptance_criteria":"\n- EventType enum with all event types\n- EventSeverity enum defined\n- AgentEvent type with all required fields\n- Specific data structures for each event type\n- EventStore interface defined\n- EventFilter for querying\n- All types exported and documented\n- JSON serialization works for Data field\n","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T01:26:10.608762-07:00","updated_at":"2025-10-15T18:12:44.792979-07:00","closed_at":"2025-10-15T18:12:44.792979-07:00","dependencies":[{"issue_id":"vc-104","depends_on_id":"vc-103","type":"parent-child","created_at":"2025-10-15T01:26:10.610206-07:00","created_by":"stevey"}]}
{"id":"vc-105","title":"Implement OutputParser with pattern-based event extraction","description":"Parse agent output lines and extract structured events using regex patterns","acceptance_criteria":"Pattern matchers for each event type, Real-time parsing as output arrives, Extract relevant data fields, Classify event severity, Handle multi-line events, Unit tests with sample output","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:26:23.300334-07:00","updated_at":"2025-10-15T19:47:15.52493-07:00","dependencies":[{"issue_id":"vc-105","depends_on_id":"vc-103","type":"parent-child","created_at":"2025-10-15T01:26:23.300745-07:00","created_by":"stevey"},{"issue_id":"vc-105","depends_on_id":"vc-104","type":"blocks","created_at":"2025-10-15T01:26:23.300981-07:00","created_by":"stevey"}]}
{"id":"vc-106","title":"Add agent_events table to storage layer","description":"Create database schema and storage implementation for agent events","acceptance_criteria":"agent_events table in schema, SQLite implementation, Indexes for fast querying, JSON storage for Data field, Query by issue/type/severity/time, Unit tests for storage operations","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T01:26:23.314917-07:00","updated_at":"2025-10-15T19:07:05.782818-07:00","closed_at":"2025-10-15T19:07:05.782818-07:00","dependencies":[{"issue_id":"vc-106","depends_on_id":"vc-103","type":"parent-child","created_at":"2025-10-15T01:26:23.315374-07:00","created_by":"stevey"},{"issue_id":"vc-106","depends_on_id":"vc-104","type":"blocks","created_at":"2025-10-15T01:26:23.315659-07:00","created_by":"stevey"}]}
{"id":"vc-107","title":"Integrate OutputParser into agent output capture","description":"Stream agent output through OutputParser and store events in real-time","acceptance_criteria":"captureOutput() parses lines into events, Events stored immediately, Both raw output and events captured, No performance degradation, Integration test with agent execution","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:26:23.329413-07:00","updated_at":"2025-10-15T19:48:11.118442-07:00","dependencies":[{"issue_id":"vc-107","depends_on_id":"vc-103","type":"parent-child","created_at":"2025-10-15T01:26:23.329861-07:00","created_by":"stevey"},{"issue_id":"vc-107","depends_on_id":"vc-105","type":"blocks","created_at":"2025-10-15T01:26:23.330081-07:00","created_by":"stevey"},{"issue_id":"vc-107","depends_on_id":"vc-106","type":"blocks","created_at":"2025-10-15T01:26:23.330326-07:00","created_by":"stevey"}]}
{"id":"vc-108","title":"Code Review Workflow","description":"Implement automated code review workflow where each code change triggers review issue creation, review-only agent mode, and automatic filing of fix issues","design":"After agent completes code task:\n1. Auto-create review issue as child\n2. Spawn review-only agent (can't modify files)\n3. Reviewer files blocking issues for problems found\n4. High-priority fixes block task completion\n5. Review findings include code context (diff/patch)","acceptance_criteria":"Auto-create review issues after code changes, Review-only agent mode implemented, Reviewer can file issues but not change code, Fix issues block original task, Code context (diff) included in fix issues","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-15T01:26:56.738002-07:00","updated_at":"2025-10-15T19:47:14.03978-07:00"}
{"id":"vc-109","title":"Implement review-only agent mode","description":"Add ReadOnly flag to AgentConfig that prevents file modifications","acceptance_criteria":"AgentConfig.ReadOnly flag, Prompt includes READ-ONLY instruction, Sandbox mounted read-only if possible, Reviewer can inspect but not modify, Test: verify agent can't write files","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:26:56.80163-07:00","updated_at":"2025-10-15T19:47:12.679871-07:00","dependencies":[{"issue_id":"vc-109","depends_on_id":"vc-108","type":"parent-child","created_at":"2025-10-15T01:26:56.802026-07:00","created_by":"stevey"}]}
{"id":"vc-11","title":"Design and implement executor_instances table","description":"Create the executor_instances table to track which executor instances are running. This enables multi-executor coordination and stale instance cleanup.","design":"Schema: instance_id (TEXT PK), hostname (TEXT), pid (INTEGER), status (running/stopped), started_at (DATETIME), last_heartbeat (DATETIME), version (TEXT), metadata (JSON). Add to Storage interface: RegisterInstance(), UpdateHeartbeat(), GetActiveInstances(), CleanupStaleInstances(). Implement in SQLite backend. Add indexes on status and last_heartbeat.","acceptance_criteria":"- executor_instances table created in schema.go\\n- Storage interface methods added\\n- SQLite implementation complete\\n- Registration and heartbeat functions work\\n- Stale instance cleanup logic implemented\\n- Schema documented with comments\\n- Basic unit tests pass","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-13T21:22:26.6102-07:00","updated_at":"2025-10-15T11:52:52.195521-07:00","closed_at":"2025-10-13T23:20:23.133979-07:00","dependencies":[{"issue_id":"vc-11","depends_on_id":"vc-5","type":"parent-child","created_at":"2025-10-13T21:22:52.237887-07:00","created_by":"stevey"},{"issue_id":"vc-11","depends_on_id":"vc-10","type":"blocks","created_at":"2025-10-13T21:22:53.549651-07:00","created_by":"stevey"}]}
{"id":"vc-110","title":"Auto-create code review issues after task completion","description":"Results processor creates review child issue when code changes detected","acceptance_criteria":"Detect code changes via git diff, Auto-create review issue as child, Review issue blocks parent completion, Review issue assigned to review agent, Include git diff in review issue context","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:26:56.964547-07:00","updated_at":"2025-10-15T19:47:43.57404-07:00","dependencies":[{"issue_id":"vc-110","depends_on_id":"vc-108","type":"parent-child","created_at":"2025-10-15T01:26:56.966552-07:00","created_by":"stevey"},{"issue_id":"vc-110","depends_on_id":"vc-109","type":"blocks","created_at":"2025-10-15T01:26:56.966801-07:00","created_by":"stevey"}]}
{"id":"vc-111","title":"Implement review findings extraction and issue filing","description":"Parse reviewer output and auto-file blocking issues for each finding","acceptance_criteria":"Extract review findings from agent output, Create blocking issue for each finding, Link findings to parent task, Include code context in finding issues, Prioritize findings (P0 for critical)","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:26:57.014824-07:00","updated_at":"2025-10-15T19:47:43.600562-07:00","dependencies":[{"issue_id":"vc-111","depends_on_id":"vc-108","type":"parent-child","created_at":"2025-10-15T01:26:57.01522-07:00","created_by":"stevey"},{"issue_id":"vc-111","depends_on_id":"vc-110","type":"blocks","created_at":"2025-10-15T01:26:57.015498-07:00","created_by":"stevey"}]}
{"id":"vc-112","title":"Add review completion gate","description":"Task can't complete until review passed and fixes resolved","acceptance_criteria":"Review issue must be closed, All blocking fixes must be resolved, Quality gate enforces review completion, Can waive review for trivial changes","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:26:57.028469-07:00","updated_at":"2025-10-15T19:47:43.613233-07:00","dependencies":[{"issue_id":"vc-112","depends_on_id":"vc-108","type":"parent-child","created_at":"2025-10-15T01:26:57.029321-07:00","created_by":"stevey"},{"issue_id":"vc-112","depends_on_id":"vc-111","type":"blocks","created_at":"2025-10-15T01:26:57.029572-07:00","created_by":"stevey"}]}
{"id":"vc-113","title":"Mission Orchestration and Middle Loop","description":"Implement outer/middle loop workflow: missions broken into phases, phases broken into tasks, with human approval gates and AI-driven planning","design":"Three-tier workflow:\nOUTER: Mission created with top-level goal\nMIDDLE: AI generates phased implementation plan, creates child epics with dependencies\nINNER: Each phase broken into granular tasks (existing executor loop)\n\nKey features:\n- AI planning phase generates work breakdown\n- Optional human approval gates\n- Phase-level sandboxes\n- Context flows: mission → phase → task\n- Epic completion triggers next phase","acceptance_criteria":"Can create mission (outer loop), AI generates phase breakdown, Phases created as child epics with deps, Human approval gate optional, Executor processes tasks within phase, Epic completion logic works, Context propagates through layers","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-10-15T01:27:18.030387-07:00","updated_at":"2025-10-15T18:09:33.817021-07:00","closed_at":"2025-10-15T18:09:33.817021-07:00"}
{"id":"vc-114","title":"Design mission planning types and interfaces","description":"Define Mission, Phase, and planner interfaces","acceptance_criteria":"Mission type (extends Issue), Phase type, MissionPlanner interface, PlanningContext structure, Generated plan validation","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T01:27:18.04239-07:00","updated_at":"2025-10-15T14:30:45.729125-07:00","closed_at":"2025-10-15T14:30:45.729125-07:00","dependencies":[{"issue_id":"vc-114","depends_on_id":"vc-113","type":"parent-child","created_at":"2025-10-15T01:27:18.04323-07:00","created_by":"stevey"}]}
{"id":"vc-115","title":"Implement AI-driven phase planning","description":"AI supervisor generates phased breakdown of mission into child epics","design":"AI-driven phase planning: Take mission description and generate structured work breakdown.\n\n## Approach\nUse Claude/GPT-4 to:\n1. Analyze mission goal and constraints\n2. Break down into logical phases/epics\n3. For each phase: title, description, design notes, acceptance criteria\n4. Identify dependencies between phases\n5. Return structured JSON with phase definitions\n\n## Similar to beads gh-9 LLM Converter\nThis is the same concept as the LLM markdown converter discussed in steveyegge/beads#9:\n- Take unstructured/semi-structured input (mission description)\n- Use AI to extract structure and dependencies\n- Generate proper bd issues with metadata\n\n## Prompt Template\n```\nYou are helping plan a software development mission. Break this down into logical phases.\n\nMission: {mission.title}\nDescription: {mission.description}\nConstraints: {constraints}\n\nGenerate a phased implementation plan as JSON:\n{\n  \"phases\": [\n    {\n      \"title\": \"Phase 1: Foundation\",\n      \"description\": \"...\",\n      \"design\": \"...\",\n      \"acceptance_criteria\": \"...\",\n      \"estimated_days\": 5,\n      \"depends_on\": []\n    },\n    ...\n  ]\n}\n\nMake phases:\n- Small enough to complete in 1-2 weeks\n- Logically ordered with clear dependencies\n- Specific and actionable\n```\n\n## Output Processing\n1. Parse JSON response\n2. Create epic for each phase\n3. Set up parent-child deps (phase -\u003e mission)\n4. Set up blocks deps between phases\n5. Return plan summary for human approval","acceptance_criteria":"Calls AI with mission context, Extracts phase list from response, Creates child epics with descriptions, Sets up dependencies between phases, Validates plan completeness","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T01:27:18.053861-07:00","updated_at":"2025-10-15T14:50:03.32027-07:00","closed_at":"2025-10-15T14:50:03.32027-07:00","dependencies":[{"issue_id":"vc-115","depends_on_id":"vc-113","type":"parent-child","created_at":"2025-10-15T01:27:18.054256-07:00","created_by":"stevey"},{"issue_id":"vc-115","depends_on_id":"vc-114","type":"blocks","created_at":"2025-10-15T01:27:18.054464-07:00","created_by":"stevey"}]}
{"id":"vc-116","title":"Add human approval gate for mission plans","description":"Optional review/approval step before executing plan","acceptance_criteria":"Displays generated plan to user, Allows approval/rejection/modification, Stores approval decision, Can skip approval with flag, REPL command for plan review","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T01:27:18.066085-07:00","updated_at":"2025-10-15T16:34:48.060505-07:00","closed_at":"2025-10-15T16:34:48.060505-07:00","dependencies":[{"issue_id":"vc-116","depends_on_id":"vc-113","type":"parent-child","created_at":"2025-10-15T01:27:18.066456-07:00","created_by":"stevey"},{"issue_id":"vc-116","depends_on_id":"vc-115","type":"blocks","created_at":"2025-10-15T01:27:18.066665-07:00","created_by":"stevey"}]}
{"id":"vc-117","title":"Implement epic completion triggers next phase","description":"When phase epic completes, automatically unblock next phase","acceptance_criteria":"Epic completion updates dependencies, Next phase becomes ready, Executor picks up next phase work, Mission progress tracked, All phases closed = mission complete","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T01:27:18.078273-07:00","updated_at":"2025-10-15T17:53:24.156701-07:00","closed_at":"2025-10-15T17:53:24.156701-07:00","dependencies":[{"issue_id":"vc-117","depends_on_id":"vc-113","type":"parent-child","created_at":"2025-10-15T01:27:18.07865-07:00","created_by":"stevey"},{"issue_id":"vc-117","depends_on_id":"vc-115","type":"blocks","created_at":"2025-10-15T01:27:18.078854-07:00","created_by":"stevey"}]}
{"id":"vc-118","title":"Git Operations Integration","description":"Orchestrate git operations at executor level: commits, rebases, branch management, merge conflict detection and recovery","design":"Features:\n- Auto-commit after task completion\n- Rebase handling with failure recovery\n- Merge conflict detection spawns fix issues\n- Branch management in sandboxes\n- Commit message generation via AI\n- Git workflow gates (tests before commit)","acceptance_criteria":"Auto-commit on task success, AI-generated commit messages, Rebase operation support, Merge conflict detection, Conflict resolution spawns issues, Git state tracked in events, Integration with sandbox branches","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-10-15T01:27:36.309577-07:00","updated_at":"2025-10-15T19:22:49.710163-07:00","closed_at":"2025-10-15T19:22:49.710163-07:00"}
{"id":"vc-119","title":"Implement auto-commit with AI-generated messages","description":"After task completion, commit changes with AI-generated message","acceptance_criteria":"Detect uncommitted changes, Generate commit message via AI, Include issue ID in message, Add co-author metadata, Respects .gitignore, Only commits relevant changes","notes":"Implemented auto-commit functionality:\n- Created internal/git/ module with GitOperations interface\n- Implemented git status detection, commit operations, and AI-based commit message generation\n- Added ExecutionStateCommitting to types\n- Integrated auto-commit into ResultsProcessor (runs after quality gates pass)\n- Includes co-author metadata (Claude)\n- Respects .gitignore (via git add -A)\n- Comprehensive integration tests\n- All acceptance criteria met","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T01:27:36.35848-07:00","updated_at":"2025-10-15T12:33:55.009001-07:00","closed_at":"2025-10-15T12:33:55.009001-07:00","dependencies":[{"issue_id":"vc-119","depends_on_id":"vc-118","type":"parent-child","created_at":"2025-10-15T01:27:36.359689-07:00","created_by":"stevey"}]}
{"id":"vc-12","title":"Design and implement issue_execution_state table","description":"Create the issue_execution_state table for checkpoint/resume support. This enables executors to save progress and resume after interruption.","design":"Schema: issue_id (TEXT FK), executor_instance_id (TEXT FK), state (claimed/assessing/executing/analyzing/gates/completed), checkpoint_data (JSON), started_at (DATETIME), updated_at (DATETIME). Add atomic ClaimIssue() that inserts execution_state row and updates issue status. Add SaveCheckpoint(), GetCheckpoint(), ResumeFromCheckpoint(). State machine: claimed → assessing → executing → analyzing → gates → completed.","acceptance_criteria":"- issue_execution_state table created in schema.go\\n- Atomic ClaimIssue() prevents double-claiming\\n- SaveCheckpoint()/GetCheckpoint() work with JSON data\\n- State transitions enforced\\n- Foreign keys to issues and executor_instances\\n- Schema documented\\n- Unit tests for claim and checkpoint operations","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-13T21:22:27.670681-07:00","updated_at":"2025-10-15T11:52:52.198239-07:00","closed_at":"2025-10-13T23:38:20.208109-07:00","dependencies":[{"issue_id":"vc-12","depends_on_id":"vc-5","type":"parent-child","created_at":"2025-10-13T21:22:52.242491-07:00","created_by":"stevey"},{"issue_id":"vc-12","depends_on_id":"vc-11","type":"blocks","created_at":"2025-10-13T21:22:53.554608-07:00","created_by":"stevey"}]}
{"id":"vc-120","title":"Add rebase operation with failure handling","description":"Support rebasing sandbox branch with conflict detection","acceptance_criteria":"Rebase against base branch, Detect merge conflicts, Create issue for conflicts, Include conflict details in issue, Abort rebase gracefully on failure, Track rebase state in sandbox","notes":"Code review fixes applied:\n\nCritical bugs fixed:\n1. Fixed hasConflicts() method to only detect actual merge conflicts (not all modified files)\n   - Now uses git diff --diff-filter=U to specifically check for unmerged paths\n   - Returns bool + error for proper error handling\n2. Added context parameter to getConflictedFiles() for proper cancellation support\n\nImprovements:\n3. Enhanced continue error handling to distinguish between:\n   - No rebase in progress (error)\n   - Still has conflicts (expected state, not error)\n   - Other errors (unexpected failures)\n4. Added test for continue success path after resolving conflicts\n\nTest results:\n- All 6 test cases passing (was 5, added 1 new)\n- Full test suite: PASS (2.122s)\n- Build: SUCCESS\n\nReady for final approval.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T01:27:36.376653-07:00","updated_at":"2025-10-15T13:11:46.041164-07:00","closed_at":"2025-10-15T13:05:39.543355-07:00","dependencies":[{"issue_id":"vc-120","depends_on_id":"vc-118","type":"parent-child","created_at":"2025-10-15T01:27:36.377067-07:00","created_by":"stevey"},{"issue_id":"vc-120","depends_on_id":"vc-119","type":"blocks","created_at":"2025-10-15T01:27:36.37743-07:00","created_by":"stevey"}]}
{"id":"vc-121","title":"Implement merge conflict resolution workflow","description":"When conflicts detected, spawn agent to resolve them","acceptance_criteria":"Parse conflict markers, Create resolution issue, Include both sides of conflict, Agent can resolve interactively, Validates resolution compiles/passes tests, Can escalate to human if stuck","notes":"Code review fixes applied:\n\nSecurity:\n- Fixed path traversal vulnerability (CRITICAL)\n- Added path validation using filepath.Join and bounds checking\n- Prevents access to files outside repository\n\nCode quality:\n- Added conflict marker constants\n- Replaced hardcoded strings throughout\n\nError handling:\n- Validation for incomplete conflict markers\n- Validation for nested/malformed markers\n- parseConflictMarkers now returns errors for invalid input\n\nTests:\n- Added 4 new test cases (12 total, was 8)\n- Path traversal prevention tests\n- Incomplete/malformed marker tests\n- All tests passing (2.154s)\n\nImplementation is production-ready and secure.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T01:27:36.390682-07:00","updated_at":"2025-10-15T14:16:57.466158-07:00","closed_at":"2025-10-15T13:50:48.280978-07:00","dependencies":[{"issue_id":"vc-121","depends_on_id":"vc-118","type":"parent-child","created_at":"2025-10-15T01:27:36.391049-07:00","created_by":"stevey"},{"issue_id":"vc-121","depends_on_id":"vc-120","type":"blocks","created_at":"2025-10-15T01:27:36.391252-07:00","created_by":"stevey"}]}
{"id":"vc-122","title":"Add git operation event tracking","description":"Track all git operations as events for activity feed","acceptance_criteria":"Detect git commands in output, Parse git operation type, Extract commit hashes/branches, Store as GitOperation events, Activity feed shows git timeline","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T01:27:36.404017-07:00","updated_at":"2025-10-15T19:07:06.943225-07:00","closed_at":"2025-10-15T19:07:06.943225-07:00","dependencies":[{"issue_id":"vc-122","depends_on_id":"vc-118","type":"parent-child","created_at":"2025-10-15T01:27:36.405114-07:00","created_by":"stevey"},{"issue_id":"vc-122","depends_on_id":"vc-104","type":"blocks","created_at":"2025-10-15T01:27:36.405475-07:00","created_by":"stevey"}]}
{"id":"vc-123","title":"Activity Feed and Progress Monitoring","description":"Real-time activity feed showing all agent actions, events, and progress across missions and tasks","acceptance_criteria":"Web UI or CLI showing live events, Filter by mission/task/event type, Show progress indicators, Display agent status, Event timeline visualization","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-15T01:27:52.590439-07:00","updated_at":"2025-10-15T11:52:52.19902-07:00"}
{"id":"vc-124","title":"Behavioral Watchdog System","description":"Detect anomalous agent behavior and intervene: infinite loops, thrashing, regressions, dangerous operations","acceptance_criteria":"Detect behavior anomalies, Generate watchdog alerts, Can pause/kill misbehaving agents, Alert on dangerous git ops, Configurable thresholds, Human escalation","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-15T01:27:52.637451-07:00","updated_at":"2025-10-15T11:52:52.199282-07:00"}
{"id":"vc-125","title":"Parallel Swarming Support","description":"Execute multiple independent tasks in parallel with isolated sandboxes","acceptance_criteria":"Multiple executors run concurrently, Each gets own sandbox, No sandbox interference, Coordinate via database locks, Scale to N workers, Resource limits enforced","status":"open","priority":2,"issue_type":"epic","created_at":"2025-10-15T01:27:52.650636-07:00","updated_at":"2025-10-15T11:52:52.199495-07:00"}
{"id":"vc-126","title":"REPL Command: Start Mission","description":"Add 'mission' command to REPL to start top-level mission workflow","acceptance_criteria":"Can create mission from REPL, Triggers AI planning, Shows generated plan, Starts execution, Integrates with continue command","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:27:52.662526-07:00","updated_at":"2025-10-15T19:47:45.854315-07:00","dependencies":[{"issue_id":"vc-126","depends_on_id":"vc-9","type":"parent-child","created_at":"2025-10-15T01:27:52.663122-07:00","created_by":"stevey"},{"issue_id":"vc-126","depends_on_id":"vc-113","type":"blocks","created_at":"2025-10-15T01:27:52.663342-07:00","created_by":"stevey"}]}
{"id":"vc-127","title":"Enhanced Quality Gates","description":"Expand quality gates beyond lint: security scans, complexity analysis, test coverage","acceptance_criteria":"Security vulnerability scanning, Code complexity metrics, Test coverage requirements, Performance regression detection, Configurable gate policies","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:27:52.676075-07:00","updated_at":"2025-10-15T11:52:52.199903-07:00"}
{"id":"vc-128","title":"Comprehensive Integration Tests","description":"End-to-end integration tests for full dogfooding workflow","acceptance_criteria":"Test: Create mission → plan → execute → review → commit, Test: Sandbox isolation, Test: Error recovery and resume, Test: Quality gate blocking, Test: Multi-task coordination, All tests pass in CI","status":"in_progress","priority":0,"issue_type":"task","created_at":"2025-10-15T01:27:52.688646-07:00","updated_at":"2025-10-15T11:52:52.200094-07:00","closed_at":"2025-10-15T11:47:19.78307-07:00"}
{"id":"vc-129","title":"Create LLM-driven markdown/text to issues converter example","description":"Create example script showing how to use LLM (Claude/GPT) to convert free-form markdown or text into structured bd issues, similar to beads md2jsonl.py but using AI for flexible parsing.","design":"\nInspired by discussion in steveyegge/beads#9 about using LLMs instead of deterministic parsing.\n\n## Approach\nCreate examples/llm-issue-generator/ with:\n1. Python script that takes free-form text/markdown\n2. Uses Claude API to analyze and extract issues\n3. Generates JSONL output for bd import\n4. Heuristically identifies dependencies\n\n## Use Cases\n- Brain dump to issue breakdown\n- Mission planning (feeds into vc-115)\n- Converting meeting notes to tasks\n- Parsing requirements docs\n\n## Prompt Strategy\n```\nAnalyze this text and extract actionable issues:\n{input_text}\n\nReturn JSON with:\n{\n  \"issues\": [\n    {\n      \"title\": \"...\",\n      \"description\": \"...\",\n      \"type\": \"task|feature|bug\",\n      \"priority\": 0-4,\n      \"dependencies\": [\"issue-id\"],\n      \"acceptance_criteria\": \"...\"\n    }\n  ]\n}\n```\n\n## Integration with VC\nThis same technique will be used in vc-115 for AI-driven phase planning.\n","acceptance_criteria":"\n- Example script in examples/ directory\n- Works with Claude API (ANTHROPIC_API_KEY)\n- Takes markdown/text file as input\n- Outputs JSONL for bd import\n- README with usage examples\n- Demonstrates flexible parsing vs rigid format\n","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:50:42.59482-07:00","updated_at":"2025-10-15T11:52:52.200291-07:00","dependencies":[{"issue_id":"vc-129","depends_on_id":"vc-115","type":"related","created_at":"2025-10-15T01:50:42.596277-07:00","created_by":"stevey"}]}
{"id":"vc-13","title":"Implement PostgreSQL backend","description":"Implement PostgreSQL storage backend. Port all schemas (issues, dependencies, executor tables) to PostgreSQL DDL and implement the Storage interface for postgres.","design":"Create internal/storage/postgres/ package mirroring sqlite structure. Port schema DDL to PostgreSQL (use JSONB for metadata/checkpoints). Implement all Storage interface methods. Add connection pooling with pgx. Create factory function in storage package to return correct backend based on config. Test switching between backends.","acceptance_criteria":"- postgres package created with full Storage implementation\\n- All schemas ported to PostgreSQL DDL\\n- Connection pooling configured\\n- Backend factory function works\\n- Can switch between SQLite and PostgreSQL via config\\n- All basic operations work on PostgreSQL\\n- Connection lifecycle handled correctly","status":"in_progress","priority":0,"issue_type":"task","created_at":"2025-10-13T21:22:29.436751-07:00","updated_at":"2025-10-15T11:52:52.200474-07:00","closed_at":"2025-10-14T13:01:05.011403-07:00","dependencies":[{"issue_id":"vc-13","depends_on_id":"vc-5","type":"parent-child","created_at":"2025-10-13T21:22:52.247016-07:00","created_by":"stevey"},{"issue_id":"vc-13","depends_on_id":"vc-12","type":"blocks","created_at":"2025-10-13T21:22:53.559463-07:00","created_by":"stevey"},{"issue_id":"vc-13","depends_on_id":"vc-16","type":"parent-child","created_at":"2025-10-13T23:48:30.576326-07:00","created_by":"stevey"},{"issue_id":"vc-13","depends_on_id":"vc-17","type":"parent-child","created_at":"2025-10-13T23:48:30.581578-07:00","created_by":"stevey"},{"issue_id":"vc-13","depends_on_id":"vc-18","type":"parent-child","created_at":"2025-10-13T23:48:30.586692-07:00","created_by":"stevey"},{"issue_id":"vc-13","depends_on_id":"vc-19","type":"parent-child","created_at":"2025-10-13T23:48:30.591391-07:00","created_by":"stevey"},{"issue_id":"vc-13","depends_on_id":"vc-20","type":"parent-child","created_at":"2025-10-13T23:48:30.596101-07:00","created_by":"stevey"},{"issue_id":"vc-13","depends_on_id":"vc-21","type":"parent-child","created_at":"2025-10-13T23:48:30.600945-07:00","created_by":"stevey"}]}
{"id":"vc-130","title":"Enhanced gate blocking integration test with real executor loop","description":"Current TestQualityGateBlocking manually flips execution states and simulates gate results. It doesn't exercise the actual executor loop or verify that the storage/executor enforces blocking behavior.\n\nFrom code review oracle:\n\"Quality gate blocking test should:\n1. Arrange a failing gate (stub or config) and transition to ExecutionStateGates\n2. Assert: cannot set StatusClosed while gate fails; verify error or block is recorded\n3. 'Fix' input; re-run gates; assert the gate passes, state transitions to Completed, then close succeeds\n4. If gates are executor-driven (not store), ensure the test triggers the executor path (Start loop with short poll interval) rather than only manually flipping states\"\n\nCurrent test only validates that we can track gate pass/fail results and manually prevent completion. Real test should start the executor with a pluggable/stubbed gate provider that can be controlled from the test.","design":"## Approach\n\n1. Add pluggable gate provider interface to executor config\n2. Create test gate provider that can be programmatically controlled (fail on first run, pass on second)\n3. Start executor with short poll interval and test gate provider\n4. Create issue and let executor claim and process it automatically\n5. Verify executor blocks at gates state when gate fails\n6. \"Fix\" the gate (flip test provider to pass mode)\n7. Trigger re-evaluation (could be automatic retry or explicit signal)\n8. Verify executor completes and closes issue\n\n## Alternative: Store-Level Blocking\n\nIf the store is supposed to enforce gate blocking (not just executor), add store methods like:\n- `CanTransitionToCompleted(ctx, issueID) (bool, error)` - checks if all gates passed\n- Update state transition validation to enforce gate checks\n\nThen test can verify store rejects invalid transitions.","acceptance_criteria":"- Executor loop integration test with real gate provider\n- Test demonstrates blocking on failed gates\n- Test demonstrates unblocking after gate passes\n- Documents whether blocking is executor-enforced or store-enforced\n- Test exercises actual executor.Start() flow, not manual state flipping","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T11:52:52.24311-07:00","updated_at":"2025-10-15T11:52:52.24311-07:00"}
{"id":"vc-131","title":"Make git diff inclusion configurable in auto-commit","description":"Currently auto-commit skips including git diff in the AI prompt (internal/executor/results.go:419-420). Including the diff would give AI better context for commit messages, but increases prompt size.\n\nAdd AutoCommitConfig with:\n- IncludeDiff bool (default: false)\n- MaxDiffChars int (default: 10000)\n\nThis allows projects to opt-in to richer commit messages when appropriate.","acceptance_criteria":"Config struct defined, Diff included when enabled, Respects MaxDiffChars limit, Defaults maintain current behavior","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-15T12:51:26.791511-07:00","updated_at":"2025-10-15T12:51:26.791511-07:00"}
{"id":"vc-132","title":"Add unit tests for MessageGenerator","description":"No unit tests exist for git.MessageGenerator.GenerateCommitMessage() (internal/git/message.go). Should add tests that:\n\n- Mock the Anthropic API response\n- Verify prompt construction includes issue context\n- Test response parsing edge cases\n- Verify retry logic works correctly\n- Test error handling\n\nThis improves confidence in the AI integration layer.","acceptance_criteria":"Unit tests added for GenerateCommitMessage, Mock Anthropic client responses, Test prompt construction, Test error scenarios, All tests pass","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-15T12:54:41.121979-07:00","updated_at":"2025-10-15T12:54:41.121979-07:00"}
{"id":"vc-133","title":"Improve git status parsing for edge cases","description":"Current git status parsing in internal/git/git.go:66-87 doesn't handle all edge cases:\n\n1. Renamed files: 'R  old -\u003e new' format not parsed correctly\n2. Submodules: May have different status codes\n3. Merge conflicts: Special status codes during conflicts\n\nImprove parsing to:\n- Extract new filename from renames (currently gets 'old -\u003e new')\n- Handle submodule status codes\n- Document unsupported cases\n\nPrevents incorrect file lists in commit metadata.","acceptance_criteria":"Renamed files parsed correctly, Submodule status handled, Edge cases documented, Tests cover edge cases, Existing tests still pass","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-15T12:54:50.042537-07:00","updated_at":"2025-10-15T12:54:50.042537-07:00"}
{"id":"vc-134","title":"Cache git binary path lookup","description":"NewGit() calls exec.LookPath('git') on every instantiation (internal/git/git.go:16-26). This is unnecessary overhead.\n\nOptions:\n1. Cache path at package level (simple but global state)\n2. Accept gitPath as optional parameter (more flexible)\n3. Use sync.Once for lazy init (thread-safe)\n\nRecommendation: Accept optional gitPath parameter, default to LookPath if empty. This enables testing with custom git paths and improves performance.","acceptance_criteria":"Git path lookup cached or parameterized, Performance improved, Backward compatible, Tests still pass, Custom git path supported for testing","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-15T12:54:59.945844-07:00","updated_at":"2025-10-15T12:54:59.945844-07:00"}
{"id":"vc-135","title":"Fix race condition in pendingPlans global map","description":"The pendingPlans map in internal/repl/approval.go:15 is a global mutable map with no mutex protection. This causes race conditions if multiple REPL commands run concurrently or in future async operations.","design":"Options:\n1. Add sync.RWMutex protection around all map accesses\n2. Move pendingPlans to REPL struct (better encapsulation)\n3. Use sync.Map for concurrent access\n\nRecommended: Move to REPL struct as it provides better encapsulation and lifecycle management.","acceptance_criteria":"Pending plans map is either mutex-protected or moved to REPL struct, Race detector passes with -race flag, Multiple concurrent plan operations work correctly","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-10-15T17:04:00.028874-07:00","updated_at":"2025-10-15T17:10:01.258223-07:00","closed_at":"2025-10-15T17:10:01.258223-07:00"}
{"id":"vc-136","title":"Add transaction/rollback for phase creation","description":"CreatePhasesFromPlan in internal/mission/orchestrator.go:108 has no rollback on partial failure. If phase 3 creation fails, phases 1-2 are already created, leaving the database inconsistent with orphaned phase issues.","design":"Options:\n1. Use database transactions (requires transaction support in storage layer)\n2. Add cleanup-on-error: track created phases and delete them if later phase fails\n3. Document the behavior and add recovery mechanism\n\nRecommended: Add cleanup-on-error as immediate fix, then add proper transaction support in storage layer.","acceptance_criteria":"Phase creation either fully succeeds or fully rolls back, No orphaned phase issues in database on error, Test case for partial failure scenario passes","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-10-15T17:04:10.263089-07:00","updated_at":"2025-10-15T17:11:59.961533-07:00","closed_at":"2025-10-15T17:11:59.961533-07:00"}
{"id":"vc-137","title":"Fix phase dependency validation logic","description":"Line 146 of internal/mission/orchestrator.go has incorrect validation: 'depPhaseNum \u003e len(phaseIDs)'. The phaseIDs array is still being built, so at phase 3 only 2 IDs exist, causing valid dependencies on earlier phases to incorrectly fail.","design":"Change validation to check against total phase count or current phase number instead of phaseIDs length:\n\nif depPhaseNum \u003c 1 || depPhaseNum \u003e= plannedPhase.PhaseNumber {\n    return phaseIDs, fmt.Errorf(...)\n}\n\nThis correctly validates that phases can only depend on earlier phases.","acceptance_criteria":"Phase dependencies validate correctly, Phase can depend on any earlier phase number, Test with 3+ phases and dependencies passes","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-15T17:04:20.030505-07:00","updated_at":"2025-10-15T17:51:12.6531-07:00","closed_at":"2025-10-15T17:51:12.6531-07:00"}
{"id":"vc-138","title":"Phase should inherit mission priority","description":"Line 118 of internal/mission/orchestrator.go hardcodes phase priority to 0 (P0). The comment says 'inherit mission priority' but doesn't actually do it. All phases get P0 regardless of mission priority.","design":"Change:\nPriority: 0, // Phases inherit mission priority\n\nTo:\nPriority: mission.Priority, // Inherit from parent mission\n\nThis requires passing the mission object to CreatePhasesFromPlan or looking it up.","acceptance_criteria":"Phases inherit parent mission priority, P2 mission creates P2 phases, Test verifies priority inheritance","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-15T17:04:29.485454-07:00","updated_at":"2025-10-15T17:51:13.967865-07:00","closed_at":"2025-10-15T17:51:13.967865-07:00"}
{"id":"vc-139","title":"Update Mission metadata on approval","description":"ApprovePlan in internal/mission/orchestrator.go only adds a comment but doesn't update Mission.ApprovedAt and Mission.ApprovedBy fields. This means Mission.IsApproved() will return false even after approval.","design":"After adding approval comment, update the mission issue with approval metadata:\n\nupdates := map[string]interface{}{\n    \"approved_at\": now,\n    \"approved_by\": approvedBy,\n}\no.store.UpdateIssue(ctx, missionID, updates, approvedBy)\n\nThis requires extending Mission type storage or using custom fields/metadata.","acceptance_criteria":"Mission.ApprovedAt is set after approval, Mission.ApprovedBy is set after approval, Mission.IsApproved() returns true after approval","status":"open","priority":1,"issue_type":"bug","created_at":"2025-10-15T17:04:39.644509-07:00","updated_at":"2025-10-15T17:04:39.644509-07:00"}
{"id":"vc-14","title":"Create migration and initialization scripts","description":"Create scripts/functions to initialize fresh databases and handle schema migrations for both SQLite and PostgreSQL.","design":"Add InitDatabase() function that creates all tables with current schema. Create internal/storage/migrations package. Design simple migration system: version table, sequential numbered migrations, up/down support. Create scripts/init-db.sh for CLI usage. Document bootstrap process in CLAUDE.md. Consider using golang-migrate or similar for production migrations.","acceptance_criteria":"- InitDatabase() creates all tables for both backends\\n- Migration framework designed and documented\\n- scripts/init-db.sh can bootstrap fresh database\\n- Version tracking table exists\\n- At least one test migration created and works\\n- Bootstrap process documented\\n- Works for both SQLite and PostgreSQL","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-13T21:22:30.615509-07:00","updated_at":"2025-10-15T19:48:14.772231-07:00","closed_at":"2025-10-14T13:04:42.168525-07:00","dependencies":[{"issue_id":"vc-14","depends_on_id":"vc-5","type":"parent-child","created_at":"2025-10-13T21:22:52.251938-07:00","created_by":"stevey"},{"issue_id":"vc-14","depends_on_id":"vc-13","type":"blocks","created_at":"2025-10-13T21:22:53.563882-07:00","created_by":"stevey"}]}
{"id":"vc-140","title":"Fix error handling in cmdReject","description":"cmdReject in internal/repl/approval.go deletes the pending plan even if adding the rejection comment fails (line 156). This causes data loss - the plan is gone but rejection wasn't recorded.","design":"Move the delete operation after successful comment:\n\nif err := r.store.AddComment(...); err != nil {\n    return fmt.Errorf(...)\n}\n// Only delete after successful save\ndelete(pendingPlans, missionID)\n\nAlso add mission existence validation like cmdApprove has.","acceptance_criteria":"Plan only deleted after successful rejection comment, cmdReject validates mission exists, Test for rejection with storage failure","status":"open","priority":2,"issue_type":"bug","created_at":"2025-10-15T17:04:48.546125-07:00","updated_at":"2025-10-15T17:04:48.546125-07:00"}
{"id":"vc-141","title":"Persist pending plans across REPL restarts","description":"Pending plans are stored in memory only (pendingPlans map). If REPL crashes or restarts, all pending plans awaiting approval are lost. Users must regenerate plans.","design":"Options:\n1. Store pending plans in database as draft/pending state\n2. Store in filesystem (JSON files)\n3. Store in mission issue metadata/labels\n4. Reconstruct from comments (plan JSON in approval.go:94)\n\nRecommended: Store in issue metadata or as structured comment that can be retrieved.","acceptance_criteria":"Pending plans survive REPL restart, /plan works after restart, Plans can be retrieved from storage","status":"open","priority":2,"issue_type":"feature","created_at":"2025-10-15T17:05:04.760697-07:00","updated_at":"2025-10-15T17:05:04.760697-07:00"}
{"id":"vc-142","title":"Add cleanup for stale pending plans","description":"Pending plans that are never approved or rejected stay in the pendingPlans map forever, causing a memory leak. Long-running REPL sessions will accumulate abandoned plans.","design":"Add TTL-based cleanup:\n1. Add timestamp to plan entries\n2. Background goroutine periodically cleans plans older than N hours\n3. Or clean on /plan command if plan too old\n\nAlso add /plans command to list all pending plans.","acceptance_criteria":"Stale plans cleaned up after timeout, No memory leak in long-running REPL, Command to list pending plans exists","status":"open","priority":2,"issue_type":"feature","created_at":"2025-10-15T17:05:37.487089-07:00","updated_at":"2025-10-15T17:05:37.487089-07:00"}
{"id":"vc-143","title":"Add mission approval observability and audit","description":"Missing observability for approval workflow: no logging, no metrics, no structured events. Also no easy way to query approval history except searching comments.","design":"Add:\n1. Structured events for approve/reject actions\n2. Logging at key decision points\n3. Metrics for approval latency, rejection rate\n4. Optional: separate approval_history table for easy querying\n\nThis helps with debugging and understanding mission approval patterns.","acceptance_criteria":"Approval actions logged with context, Events emitted for approve/reject, Can query approval history programmatically","status":"open","priority":3,"issue_type":"feature","created_at":"2025-10-15T17:05:47.244114-07:00","updated_at":"2025-10-15T17:05:47.244114-07:00"}
{"id":"vc-144","title":"Refactor approval state ownership","description":"Tight coupling: Orchestrator doesn't own approval state - pendingPlans lives in REPL package. This creates awkward APIs (StorePendingPlan, GetPendingPlan, ClearPendingPlan) that reach into REPL state from orchestrator.","design":"Move approval state management into Orchestrator or new ApprovalManager:\n1. Orchestrator owns pendingPlans\n2. REPL calls orchestrator methods to manage plans\n3. Cleaner separation of concerns\n4. Easier to test orchestrator independently\n5. Enables non-REPL approval workflows (API, CLI)","acceptance_criteria":"Approval state owned by orchestrator or separate manager, REPL doesn't expose global state, Orchestrator can be tested without REPL","status":"open","priority":3,"issue_type":"chore","created_at":"2025-10-15T17:06:02.680649-07:00","updated_at":"2025-10-15T17:06:02.680649-07:00"}
{"id":"vc-145","title":"Fix AgentEvent JSON serialization and add data structure helpers","description":"Critical issues with JSON serialization and type safety for AgentEvent and specific data structures","design":"1. Add JSON tags to all AgentEvent fields for consistent serialization (lowercase with underscores: id, issue_id, executor_id, etc.)\n2. Create helper methods to convert specific data structures (FileModifiedData, TestRunData, GitOperationData) to/from the Data map[string]interface{}:\n   - func (e *AgentEvent) SetFileModifiedData(data FileModifiedData)\n   - func (e *AgentEvent) GetFileModifiedData() (*FileModifiedData, error)\n   - Similar methods for TestRunData and GitOperationData\n3. Consider custom MarshalJSON for time.Duration in TestRunData to make it human-readable (e.g., '1.5s' instead of 1500000000)\n4. Add type-safe constructors that take the specific data structures instead of raw maps","acceptance_criteria":"- All AgentEvent fields have json tags with snake_case names\n- Helper methods exist to get/set all specific data structure types\n- Test coverage demonstrates type-safe usage of specific data structures\n- JSON serialization produces human-readable output\n- No direct manipulation of Data map required by users","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T18:29:43.514872-07:00","updated_at":"2025-10-15T18:29:43.514872-07:00","dependencies":[{"issue_id":"vc-145","depends_on_id":"vc-104","type":"discovered-from","created_at":"2025-10-15T18:30:55.420731-07:00","created_by":"stevey"}]}
{"id":"vc-146","title":"Add validation methods and enum helper functions","description":"Add validation and utility functions for EventType and EventSeverity enums to prevent invalid values and improve developer experience","design":"1. Add validation methods:\n   - func (et EventType) IsValid() bool - checks if EventType is one of the defined constants\n   - func (es EventSeverity) IsValid() bool - checks if EventSeverity is valid\n2. Add helper functions:\n   - func AllEventTypes() []EventType - returns all valid event types\n   - func AllEventSeverities() []EventSeverity - returns all valid severities\n   - func ParseEventType(s string) (EventType, error) - parses string to EventType with validation\n   - func ParseEventSeverity(s string) (EventSeverity, error) - parses string with validation\n3. Consider adding String() methods if custom formatting needed (though type aliases to string already have this)","acceptance_criteria":"- IsValid() methods exist for both EventType and EventSeverity\n- AllEventTypes() and AllEventSeverities() functions return complete lists\n- Parse functions validate input and return errors for invalid values\n- Tests verify validation catches invalid enum values\n- Documentation includes examples of usage","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-15T18:29:56.246059-07:00","updated_at":"2025-10-15T18:29:56.246059-07:00","dependencies":[{"issue_id":"vc-146","depends_on_id":"vc-104","type":"discovered-from","created_at":"2025-10-15T18:30:55.434679-07:00","created_by":"stevey"}]}
{"id":"vc-147","title":"Improve EventStore API design and add pagination","description":"Streamline EventStore interface, clarify EventFilter semantics, add pagination support, and define error types","design":"1. Simplify EventStore interface by removing redundant methods:\n   - Remove GetEventsByIssue (use GetEvents with filter instead)\n   - Remove GetRecentEvents (use GetEvents with filter instead)\n   - Keep only: StoreEvent, GetEvents\n2. Document EventFilter behavior:\n   - Specify that non-zero fields are ANDed together\n   - Define behavior for AfterTime \u003e BeforeTime (return error or empty results)\n   - Define behavior for Limit \u003c= 0 (unlimited or error)\n   - Add validation method: func (f EventFilter) Validate() error\n3. Add pagination to EventFilter:\n   - Add Offset field OR Cursor field for pagination\n   - Document ordering (by timestamp desc by default)\n4. Define error types:\n   - var ErrEventNotFound = errors.New(\"event not found\")\n   - var ErrInvalidFilter = errors.New(\"invalid filter\")\n   - Document which methods return which errors","acceptance_criteria":"- EventStore has only StoreEvent and GetEvents methods\n- EventFilter has clear documentation of AND semantics\n- EventFilter has Validate() method that catches invalid combinations\n- EventFilter supports pagination (Offset or Cursor)\n- Defined error types exist and are documented\n- Tests verify filter validation and error handling","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-15T18:30:11.096528-07:00","updated_at":"2025-10-15T18:30:11.096528-07:00","dependencies":[{"issue_id":"vc-147","depends_on_id":"vc-104","type":"discovered-from","created_at":"2025-10-15T18:30:55.447115-07:00","created_by":"stevey"}]}
{"id":"vc-148","title":"Add AgentEvent constructors and improve documentation","description":"Add factory functions for creating AgentEvents and improve documentation around thread safety and usage patterns","design":"1. Add constructor/factory functions:\n   - func NewAgentEvent(issueID, executorID, agentID string, eventType EventType, severity EventSeverity, message string) *AgentEvent\n     - Auto-generates UUID for ID\n     - Sets Timestamp to time.Now()\n     - Initializes empty Data map\n     - Validates inputs\n   - Specialized constructors:\n     - func NewFileModifiedEvent(issueID, executorID, agentID string, data FileModifiedData) *AgentEvent\n     - func NewTestRunEvent(issueID, executorID, agentID string, data TestRunData) *AgentEvent\n     - func NewGitOperationEvent(issueID, executorID, agentID string, data GitOperationData) *AgentEvent\n2. Improve documentation:\n   - Add package-level documentation with usage examples\n   - Document EventStore thread safety requirements\n   - Add example code in godoc format\n3. Add ID generation:\n   - Use github.com/google/uuid or similar for generating event IDs","acceptance_criteria":"- NewAgentEvent constructor exists and auto-generates IDs and timestamps\n- Specialized constructors exist for each specific data type\n- Event IDs are valid UUIDs\n- Package documentation includes usage examples\n- EventStore interface documents thread safety requirements\n- Tests verify constructors work correctly","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-15T18:30:25.32458-07:00","updated_at":"2025-10-15T18:30:25.32458-07:00","dependencies":[{"issue_id":"vc-148","depends_on_id":"vc-104","type":"discovered-from","created_at":"2025-10-15T18:30:55.458998-07:00","created_by":"stevey"}]}
{"id":"vc-149","title":"Expand test coverage for events package","description":"Add comprehensive test coverage for edge cases, validation, and round-trip serialization","design":"1. Add JSON round-trip tests:\n   - Verify Data field contents survive marshal/unmarshal\n   - Test timestamp precision preservation\n   - Test all specific data structures round-trip correctly\n2. Add edge case tests:\n   - nil Data map\n   - empty strings in required fields\n   - negative SourceLine values\n   - zero-value timestamps\n   - very large Data payloads\n3. Add validation tests:\n   - Invalid EventType and EventSeverity values\n   - EventFilter with invalid time ranges\n   - EventFilter validation edge cases\n4. Add benchmark tests:\n   - JSON marshaling performance\n   - Large event serialization\n5. Integration tests for specific data structures:\n   - FileModifiedData with various file paths (absolute, relative, special chars)\n   - TestRunData with edge case durations\n   - GitOperationData with complex argument arrays","acceptance_criteria":"- Test coverage \u003e 80% for types.go\n- All edge cases have explicit test coverage\n- Round-trip tests verify Data field integrity\n- Benchmark tests exist for serialization\n- Tests for all specific data structure types\n- Tests verify validation catches invalid inputs","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-15T18:30:39.496519-07:00","updated_at":"2025-10-15T18:30:39.496519-07:00","dependencies":[{"issue_id":"vc-149","depends_on_id":"vc-104","type":"discovered-from","created_at":"2025-10-15T18:30:55.486001-07:00","created_by":"stevey"}]}
{"id":"vc-15","title":"Integration tests for executor functionality","description":"Write integration tests validating the full executor table functionality, including multi-executor scenarios, claim/checkpoint/resume flows, and both database backends.","design":"Create internal/storage/integration_test.go. Test scenarios: 1) Multiple executors claiming different issues (no conflicts), 2) Claim race condition handling, 3) Checkpoint save and restore, 4) Stale instance cleanup, 5) Resume after interruption, 6) All above on both SQLite and PostgreSQL. Use table-driven tests to run same scenarios on both backends.","acceptance_criteria":"- Integration test file created\\n- Multi-executor claim scenarios pass\\n- Race condition tests pass (no double-claiming)\\n- Checkpoint/resume cycle works\\n- Stale instance cleanup verified\\n- All tests pass on SQLite\\n- All tests pass on PostgreSQL\\n- Test coverage documented","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-13T21:22:31.886325-07:00","updated_at":"2025-10-15T19:48:14.754271-07:00","closed_at":"2025-10-14T16:34:50.636629-07:00","dependencies":[{"issue_id":"vc-15","depends_on_id":"vc-5","type":"parent-child","created_at":"2025-10-13T21:22:52.256877-07:00","created_by":"stevey"},{"issue_id":"vc-15","depends_on_id":"vc-14","type":"blocks","created_at":"2025-10-13T21:22:53.568457-07:00","created_by":"stevey"},{"issue_id":"vc-15","depends_on_id":"vc-6","type":"blocks","created_at":"2025-10-14T13:10:14.294156-07:00","created_by":"stevey"}]}
{"id":"vc-150","title":"Improve ZFC Compliance - Remove Hardcoded Heuristics","description":"Remove hardcoded decision-making logic from orchestration layer and delegate to AI. Current violations include: epic/mission completion heuristics, output summarization, phase validation rules, and gate failure handling.","design":"Replace hardcoded policies with AI-delegated decisions:\n1. Epic/mission completion should ask AI, not count closed children\n2. Output summarization should use AI, not 'last 10 lines'\n3. Phase validation rules should come from AI planner\n4. Gate failure recovery should generate AI strategies\n5. Mission identification should use explicit typing\n\nZFC principle: ALL decisions delegated to AI, only coordination logic in orchestration layer.","acceptance_criteria":"All completion checks use AI assessment, Output summarization uses AI, Phase rules configurable/AI-validated, Gate failures generate AI recovery plans, No type inference heuristics remain, Tests verify AI delegation, Documentation updated","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-15T19:34:15.52658-07:00","updated_at":"2025-10-15T19:34:15.52658-07:00"}
{"id":"vc-151","title":"Add AI completion advisor for epics and missions","description":"Replace hardcoded 'all children closed = complete' logic with AI assessment. Ask AI if epic/mission is truly complete based on goals, not just child status counts.","design":"Add Supervisor.AssessCompletion(ctx, epic) method that:\n- Evaluates if epic objectives are met\n- Considers child statuses as input, not determinant\n- Returns completion decision + reasoning\n- Handles edge cases (blocked children, discovered work)\n\nUpdate epic.go checkAndCloseEpicIfComplete() to call AI instead of counting.\nUpdate mission orchestrator CheckMissionCompletion() similarly.\n\nExample: Epic could be 'complete enough' even with open polish tasks, or 'incomplete' despite all children closed if core goal unmet.","acceptance_criteria":"Supervisor has AssessCompletion method, Epic completion calls AI advisor, Mission completion calls AI advisor, Tests verify AI is consulted, Can handle various completion scenarios, Reasoning logged to comments","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T19:34:35.057401-07:00","updated_at":"2025-10-15T19:34:35.057401-07:00","dependencies":[{"issue_id":"vc-151","depends_on_id":"vc-150","type":"parent-child","created_at":"2025-10-15T19:35:08.068097-07:00","created_by":"stevey"}]}
{"id":"vc-152","title":"Replace output summarization with AI","description":"Replace 'last 10 lines' heuristic in results.go with AI-based summarization. AI should extract key points from agent output regardless of length or structure.","design":"Add Supervisor.SummarizeAgentOutput(ctx, fullOutput, maxLength) method.\n\nReplace extractSummary() in results.go:\n- Send full output to AI (with context about issue)\n- AI extracts: what was done, key decisions, important warnings\n- Returns concise summary suitable for comment/notification\n- Handles various output formats (test results, build logs, etc.)\n\nConsider token limits: for very large outputs, use intelligent chunking or sampling, not just truncation.","acceptance_criteria":"Supervisor has SummarizeAgentOutput method, extractSummary uses AI instead of line count, Handles outputs of varying length, Handles different output types, Tests verify quality of summaries, Performance acceptable for typical outputs","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T19:34:36.502231-07:00","updated_at":"2025-10-15T19:34:36.502231-07:00","dependencies":[{"issue_id":"vc-152","depends_on_id":"vc-150","type":"parent-child","created_at":"2025-10-15T19:35:08.081683-07:00","created_by":"stevey"}]}
{"id":"vc-153","title":"Add AI recovery strategies for quality gate failures","description":"When quality gates fail, ask AI to generate recovery strategy instead of hardcoded 'mark as blocked'. AI should recommend: retry, break into sub-tasks, escalate to human, ignore if non-critical, etc.","design":"Add Supervisor.GenerateRecoveryStrategy(ctx, issue, gateResults) method that:\n- Analyzes which gates failed and why\n- Considers issue context and priority\n- Recommends recovery approach with reasoning\n- Can suggest creating sub-issues for specific fixes\n\nUpdate gates.HandleGateResults() to:\n1. Call AI for recovery strategy\n2. Execute recommended actions (create issues, update status, etc.)\n3. Log reasoning\n\nStrategies could include:\n- 'Fix in place' - keep issue open, add gate issues as blockers\n- 'Acceptable failure' - close anyway if non-critical (with approval)\n- 'Split work' - create focused fix issues\n- 'Escalate' - flag for human review","acceptance_criteria":"Supervisor has GenerateRecoveryStrategy method, HandleGateResults uses AI strategy, Multiple recovery strategies supported, Reasoning logged to comments, Tests verify strategy generation, Can handle various gate failure scenarios","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-15T19:34:57.38772-07:00","updated_at":"2025-10-15T19:34:57.38772-07:00","dependencies":[{"issue_id":"vc-153","depends_on_id":"vc-150","type":"parent-child","created_at":"2025-10-15T19:35:08.093539-07:00","created_by":"stevey"}]}
{"id":"vc-154","title":"Make phase validation rules AI-configurable","description":"Replace hardcoded phase dependency validation (phases can only depend on earlier phases) with AI-validated or configurable rules.","design":"Options:\n1. Ask AI planner to validate phase structure during generation\n2. Store validation rules as configurable policies\n3. Delegate validation to AI supervisor\n\nRecommended approach: Add ValidatePhaseStructure() to planner interface.\n\nUpdate orchestrator.CreatePhasesFromPlan():\n- Call planner.ValidatePhaseStructure(phases) before creation\n- Planner checks dependencies make sense for the mission type\n- More flexible than hardcoded 'earlier only' rule\n- AI can allow forward deps if justified\n\nAlso remove 'epicChildren \u003e 1 = mission' heuristic:\n- Add explicit IssueSubtype field (mission, phase, normal)\n- Set during creation, not inferred from structure","acceptance_criteria":"Phase validation delegated to planner/AI, No hardcoded dependency ordering rules, IssueSubtype field added to Issue type, Mission/phase identification uses explicit typing, Tests verify flexible validation, Backward compatible with existing issues","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-15T19:34:59.419589-07:00","updated_at":"2025-10-15T19:34:59.419589-07:00","dependencies":[{"issue_id":"vc-154","depends_on_id":"vc-150","type":"parent-child","created_at":"2025-10-15T19:35:08.133796-07:00","created_by":"stevey"}]}
{"id":"vc-16","title":"Create postgres.go with connection pooling and base structure","description":"Create internal/storage/postgres/postgres.go with PostgresStorage struct, connection pooling via pgx.Pool, New() constructor, and Close() method. Set up basic structure mirroring SQLite implementation.","design":"Use pgx/v5 connection pool. Connection string: postgres://user:pass@host:port/dbname. Configure pool size, timeouts. Initialize schema on New(). Implement proper connection lifecycle.","acceptance_criteria":"- PostgresStorage struct with pgx.Pool\\n- New() constructor with connection pooling\\n- Schema initialization on startup\\n- Close() method for cleanup\\n- Connection string parsing\\n- Pool configuration (max conns, timeouts)\\n- Error handling for connection failures","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-13T23:47:34.720193-07:00","updated_at":"2025-10-15T11:52:52.201237-07:00","closed_at":"2025-10-14T00:03:08.987536-07:00"}
{"id":"vc-17","title":"Implement PostgreSQL issue operations","description":"Implement issue CRUD operations in PostgreSQL: CreateIssue, GetIssue, UpdateIssue, CloseIssue, SearchIssues. Port from SQLite implementation, converting ? placeholders to $1, $2, etc.","design":"Port issue operations from internal/storage/sqlite/sqlite.go. Use numbered placeholders ($1, $2). Handle ID generation for new issues. Implement field validation. Use transactions where appropriate. Handle NULL values correctly for optional fields (assignee, estimated_minutes, closed_at).","acceptance_criteria":"- CreateIssue() works with ID generation\\n- GetIssue() retrieves issues correctly\\n- UpdateIssue() handles partial updates\\n- CloseIssue() sets status and closed_at\\n- SearchIssues() supports filtering and pagination\\n- Event logging integrated\\n- Validation enforced\\n- Transactions used appropriately","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-13T23:47:48.249848-07:00","updated_at":"2025-10-15T11:52:52.201428-07:00","closed_at":"2025-10-14T00:05:51.918897-07:00","dependencies":[{"issue_id":"vc-17","depends_on_id":"vc-16","type":"blocks","created_at":"2025-10-13T23:48:44.920406-07:00","created_by":"stevey"}]}
{"id":"vc-18","title":"Implement PostgreSQL dependency, label, and event operations","description":"Port dependencies.go, labels.go, and events.go from SQLite to PostgreSQL. Implement AddDependency, RemoveDependency, GetDependencies, GetDependents, GetDependencyTree, DetectCycles for dependencies. AddLabel, RemoveLabel, GetLabels, GetIssuesByLabel for labels. AddComment, GetEvents for events.","design":"Port from SQLite files. Convert ? to $N. Handle foreign key constraints. Implement cycle detection algorithm. Use recursive CTEs for dependency tree queries in PostgreSQL.","acceptance_criteria":"- All dependency operations work\\n- All label operations work\\n- All event operations work\\n- Cycle detection functional\\n- Dependency tree query efficient\\n- Foreign keys enforced\\n- Event logging works","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-13T23:48:04.079317-07:00","updated_at":"2025-10-15T11:52:52.201769-07:00","closed_at":"2025-10-14T00:22:21.848492-07:00","dependencies":[{"issue_id":"vc-18","depends_on_id":"vc-17","type":"blocks","created_at":"2025-10-13T23:48:44.925689-07:00","created_by":"stevey"}]}
{"id":"vc-19","title":"Implement PostgreSQL executor instance operations","description":"Port executor_instances.go from SQLite to PostgreSQL. Implement RegisterInstance, UpdateHeartbeat, GetActiveInstances, CleanupStaleInstances. Handle JSONB metadata field.","design":"Port from sqlite/executor_instances.go. Use JSONB for metadata. Implement upsert pattern for RegisterInstance. Use timestamptz comparisons for stale detection. Handle connection pool correctly.","acceptance_criteria":"- RegisterInstance works with upsert\\n- UpdateHeartbeat updates timestamp\\n- GetActiveInstances filters by status\\n- CleanupStaleInstances handles threshold\\n- JSONB metadata handled correctly\\n- All operations tested","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-13T23:48:04.085658-07:00","updated_at":"2025-10-15T19:47:31.23253-07:00","closed_at":"2025-10-14T03:10:24.249484-07:00","dependencies":[{"issue_id":"vc-19","depends_on_id":"vc-16","type":"blocks","created_at":"2025-10-13T23:48:44.931383-07:00","created_by":"stevey"}]}
{"id":"vc-2","title":"Recursive Refinement and Follow-On Missions","description":"The core of 'Engineer-in-a-Box'. AI analyzes execution results and automatically creates follow-on issues for discovered work, punted items, and quality problems. This is what makes vc self-improving instead of just a task executor.","design":"After each issue execution, AI analyzes the result and extracts: 1) Punted work (deferred items), 2) Discovered bugs/issues, 3) Quality problems. Automatically creates child issues with 'discovered-from' dependencies. Executor processes these recursively until all work is complete.","acceptance_criteria":"- analyzeExecutionResult AI activity implemented\n- Automatic issue creation from AI analysis\n- discovered-from dependency type support\n- Punted items labeled and tracked\n- Quality issues create blocking dependencies\n- Recursive processing until convergence\n- Integration with quality gates","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-13T21:05:19.449075-07:00","updated_at":"2025-10-15T19:48:22.273626-07:00","dependencies":[{"issue_id":"vc-2","depends_on_id":"vc-1","type":"blocks","created_at":"2025-10-13T21:05:19.449939-07:00","created_by":"import"}]}
{"id":"vc-20","title":"Implement PostgreSQL execution state and ready work operations","description":"Port execution_state.go and ready.go from SQLite to PostgreSQL. Implement ClaimIssue, GetExecutionState, UpdateExecutionState, SaveCheckpoint, GetCheckpoint, ReleaseIssue for execution state. Implement GetReadyWork, GetBlockedIssues, GetStatistics for ready work queries. Use JSONB for checkpoint data.","design":"Port from SQLite. Use JSONB for checkpoint_data. Implement atomic ClaimIssue with proper PostgreSQL locking. Use views for ready_issues and blocked_issues queries. Implement statistics aggregation efficiently.","acceptance_criteria":"- ClaimIssue atomic and prevents double-claiming\\n- All execution state operations work\\n- GetReadyWork returns correct issues\\n- GetBlockedIssues identifies blocked work\\n- GetStatistics provides accurate counts\\n- JSONB checkpoint data handled\\n- State machine enforced","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-13T23:48:21.302415-07:00","updated_at":"2025-10-15T19:48:14.722765-07:00","closed_at":"2025-10-14T03:32:23.904021-07:00","dependencies":[{"issue_id":"vc-20","depends_on_id":"vc-17","type":"blocks","created_at":"2025-10-13T23:48:44.937538-07:00","created_by":"stevey"},{"issue_id":"vc-20","depends_on_id":"vc-19","type":"blocks","created_at":"2025-10-13T23:48:44.942434-07:00","created_by":"stevey"}]}
{"id":"vc-21","title":"Create storage factory function for backend selection","description":"Add factory function in internal/storage/storage.go to create correct backend (SQLite or PostgreSQL) based on Config. Enable switching between backends via configuration.","design":"Add NewStorage(config Config) (Storage, error) function. Check config.Backend field. Return sqlite.New() or postgres.New() based on backend. Handle connection string building for PostgreSQL. Validate config before creating backend.","acceptance_criteria":"- NewStorage() factory function works\\n- Selects SQLite when config.Backend='sqlite'\\n- Selects PostgreSQL when config.Backend='postgres'\\n- Config validation implemented\\n- Connection string handling correct\\n- Error messages clear\\n- Can switch backends easily","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-13T23:48:21.307824-07:00","updated_at":"2025-10-15T11:52:52.202942-07:00","closed_at":"2025-10-14T01:10:59.496449-07:00","dependencies":[{"issue_id":"vc-21","depends_on_id":"vc-16","type":"blocks","created_at":"2025-10-13T23:48:44.947254-07:00","created_by":"stevey"}]}
{"id":"vc-22","title":"Fix PostgreSQL ID generation race condition","description":"The getNextID() function in postgres.go has a race condition in multi-executor scenarios. Two executors can both read MAX(id) and get the same nextID, leading to UNIQUE constraint violations.","design":"Options: 1) Use PostgreSQL SEQUENCE for ID generation (most robust), 2) Use atomic database-side ID allocation (SELECT ... FOR UPDATE), 3) Document that each executor should have its own ID range. Recommended: PostgreSQL sequence with 'vc-' prefix using a custom function.","acceptance_criteria":"- ID generation is thread-safe across multiple executor instances\\n- No UNIQUE constraint violations possible\\n- Tests verify concurrent ID generation\\n- Performance acceptable (\u003c 5ms per ID)","status":"open","priority":1,"issue_type":"bug","created_at":"2025-10-14T00:15:22.574086-07:00","updated_at":"2025-10-15T11:52:52.203185-07:00","closed_at":"2025-10-14T11:09:57.203873-07:00"}
{"id":"vc-23","title":"Fix error handling in getNextID function","description":"The getNextID() function in postgres.go silently ignores errors. On line 134, 'if err \\!= nil \u0026\u0026 err \\!= pgx.ErrNoRows' returns 1 instead of propagating the error. Network failures, permission errors, etc. would be masked.","design":"Change logic to: 1) Check if err == pgx.ErrNoRows, return 1, 2) If any other error, return error with context, 3) Add test for error propagation.","acceptance_criteria":"- Network errors properly propagated\\n- Permission errors properly propagated\\n- Only pgx.ErrNoRows returns default ID\\n- Error messages include context\\n- Test coverage for error cases","status":"open","priority":1,"issue_type":"bug","created_at":"2025-10-14T00:15:31.139269-07:00","updated_at":"2025-10-15T11:52:52.203361-07:00","closed_at":"2025-10-14T11:12:23.843624-07:00"}
{"id":"vc-24","title":"Add context deadline checks to long-running PostgreSQL queries","description":"Long-running queries like DetectCycles and GetDependencyTree don't check ctx.Done(). If a client disconnects or times out, these queries continue running as zombies, wasting database resources.","design":"Add context deadline checks in loops that process rows. Use ctx.Err() to detect cancellation. For recursive CTEs, consider setting statement_timeout in PostgreSQL. Add integration tests with cancelled contexts.","acceptance_criteria":"- DetectCycles respects context cancellation\\n- GetDependencyTree respects context cancellation\\n- Zombie queries cleaned up on client disconnect\\n- Tests verify cancellation behavior\\n- No resource leaks on timeout","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-14T00:15:40.585971-07:00","updated_at":"2025-10-15T11:52:52.203536-07:00"}
{"id":"vc-25","title":"Add helper function for scanning issues in PostgreSQL backend","description":"The postgres.go file has duplicated issue scanning logic in SearchIssues, GetDependencies, GetDependents, and other methods. SQLite has a scanIssues() helper that should be replicated for PostgreSQL.","design":"Create scanIssues(rows pgx.Rows) ([]*types.Issue, error) helper function. Handle NULL values for closedAt, estimatedMinutes, assignee consistently. Use in all methods that return []*types.Issue.","acceptance_criteria":"- scanIssues helper function created\\n- All issue-returning methods use helper\\n- NULL handling consistent across methods\\n- Code duplication eliminated\\n- No behavior changes (tests pass)","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T00:15:51.288622-07:00","updated_at":"2025-10-15T11:52:52.205235-07:00"}
{"id":"vc-26","title":"Add connection pool metrics and observability to PostgreSQL backend","description":"The PostgreSQL connection pool (pgxpool) provides metrics like active connections, idle connections, wait time, etc. These should be exposed for monitoring and debugging production issues.","design":"Use pgxpool.Stat() to expose metrics. Options: 1) Add GetPoolStats() method to Storage interface, 2) Export metrics to Prometheus, 3) Log periodic stats. Consider adding pool exhaustion warnings.","acceptance_criteria":"- Pool stats exposed via API\\n- Active/idle connection counts available\\n- Wait time/duration tracked\\n- Pool exhaustion warnings logged\\n- Metrics helpful for debugging production issues","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T00:16:00.172988-07:00","updated_at":"2025-10-15T11:52:52.205414-07:00"}
{"id":"vc-27","title":"Fix AddLabel/RemoveLabel to check RowsAffected before recording events","description":"AddLabel and RemoveLabel in PostgreSQL backend record events even when no changes occur. AddLabel uses ON CONFLICT DO NOTHING but still records event if label already exists. RemoveLabel records event even if label doesn't exist. This creates misleading audit trail.","design":"Check CommandTag.RowsAffected() after INSERT/DELETE operations. Only record event if rows were actually modified. For AddLabel: result.RowsAffected() \u003e 0. For RemoveLabel: result.RowsAffected() \u003e 0.","acceptance_criteria":"AddLabel skips event when label already exists; RemoveLabel skips event when label doesn't exist; audit trail only shows actual changes","status":"open","priority":1,"issue_type":"bug","created_at":"2025-10-14T00:36:26.579297-07:00","updated_at":"2025-10-15T11:52:52.205578-07:00","closed_at":"2025-10-14T02:28:38.026064-07:00"}
{"id":"vc-28","title":"Fix DetectCycles N+1 query problem with bulk issue fetch","description":"DetectCycles in PostgreSQL backend has N+1 query problem at postgres.go:792-800. For each issue ID in a cycle path, it calls GetIssue() separately. A cycle with 10 issues makes 10 database round trips. This is inefficient and doesn't scale.","design":"After parsing all cycle paths and collecting unique issue IDs, make single bulk query: SELECT * FROM issues WHERE id IN ($1, $2, ...). Build map[issueID]*Issue for lookup. Then assemble cycles from map. Reduces N queries to 1.","acceptance_criteria":"DetectCycles makes single bulk query for all issues in all cycles; performance scales with unique issues not cycle count","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-10-14T00:36:43.202121-07:00","updated_at":"2025-10-15T11:52:52.205876-07:00","closed_at":"2025-10-14T00:50:03.660637-07:00"}
{"id":"vc-29","title":"Fix AddDependency race condition by moving validation into transaction","description":"AddDependency validates issue existence at postgres.go:499-513 BEFORE starting transaction at line 523. Race condition: issues could be deleted between validation and insertion, causing foreign key violations or inconsistent state. Also makes 2 extra round trips.","design":"Move GetIssue validation calls inside transaction after Begin(). Or better: remove explicit validation and rely on foreign key constraints - let database enforce referential integrity. Catch and translate FK violation errors to user-friendly messages.","acceptance_criteria":"AddDependency validation happens inside transaction; no race condition possible; foreign key violations handled gracefully","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-10-14T00:36:59.992736-07:00","updated_at":"2025-10-15T11:52:52.206046-07:00","closed_at":"2025-10-14T00:46:08.300061-07:00"}
{"id":"vc-3","title":"Watchdog and Convergence Detection","description":"Prevents infinite loops, stuck issues, and low-confidence spirals. Monitors executor progress and escalates when tasks aren't making forward progress. Should-have for production reliability.","design":"Monitor executor iterations and detect: 1) Max iterations without issue completion, 2) Repeated low-confidence AI assessments, 3) No progress within time threshold, 4) Escalation count exceeding limit. When detected, escalate to human or abort gracefully.","acceptance_criteria":"- Watchdog monitor component\n- maxIterationsWithoutProgress tracking\n- maxLowConfidenceAssessments tracking\n- minProgressCheckInterval enforcement\n- Escalation thresholds\n- Graceful abort on convergence failure\n- Logging and metrics integration","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-13T21:05:19.449174-07:00","updated_at":"2025-10-15T11:52:52.20623-07:00","dependencies":[{"issue_id":"vc-3","depends_on_id":"vc-2","type":"blocks","created_at":"2025-10-13T21:05:19.450051-07:00","created_by":"import"}]}
{"id":"vc-30","title":"Add limit validation to GetEvents in PostgreSQL backend","description":"GetEvents at postgres.go:942-945 accepts limit parameter but doesn't validate it. Negative or excessively large limits could cause issues. While integer formatting prevents SQL injection, unbounded queries are a DoS risk.","design":"Add validation: if limit \u003c 0 return error. If limit \u003e 10000 cap at 10000 or return error. Document maximum in function comment.","acceptance_criteria":"GetEvents rejects negative limits; enforces reasonable maximum; documented behavior","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T00:37:14.878662-07:00","updated_at":"2025-10-15T11:52:52.206401-07:00","closed_at":"2025-10-14T02:28:23.060423-07:00"}
{"id":"vc-31","title":"Fix GetDependencyTree truncation flag logic","description":"GetDependencyTree at postgres.go:722 sets node.Truncated = node.Depth == maxDepth. This is off-by-one: should be \u003e= maxDepth. Nodes AT maxDepth are the last level returned, so they ARE truncated (their children aren't shown). Current code would only mark imaginary 'maxDepth+1' nodes.","design":"Change line 722 from == to \u003e=. Or better: set truncated=true for nodes whose depth == maxDepth-1 AND they have children in dependencies table. This shows truncation only when children actually exist.","acceptance_criteria":"Truncation flag correct when dependency tree reaches max depth","status":"open","priority":2,"issue_type":"bug","created_at":"2025-10-14T00:37:31.476488-07:00","updated_at":"2025-10-15T11:52:52.206581-07:00","closed_at":"2025-10-14T12:51:27.951601-07:00"}
{"id":"vc-32","title":"Initialize GetLabels with empty slice instead of nil","description":"GetLabels at postgres.go:874 declares 'var labels []string' which creates nil slice. When no labels exist, returns nil instead of empty slice. Forces callers to check for nil vs empty. Go convention is to return empty slices, not nil.","design":"Change declaration to 'labels := []string{}' at postgres.go:874. Returns consistent empty slice when no labels found.","acceptance_criteria":"GetLabels returns empty slice [] instead of nil when issue has no labels","status":"open","priority":2,"issue_type":"bug","created_at":"2025-10-14T00:37:47.544436-07:00","updated_at":"2025-10-15T11:52:52.206911-07:00","closed_at":"2025-10-14T12:51:02.104427-07:00"}
{"id":"vc-33","title":"Add rows.Err() checks after iteration in PostgreSQL queries","description":"Multiple query functions in PostgreSQL backend iterate rows but don't check rows.Err() afterward. GetLabels at postgres.go:881 is one example. If iteration stops due to error, we silently return partial results. Should check rows.Err() after loop completes.","design":"After for rows.Next() loops, add: if err := rows.Err(); err \\!= nil { return nil, fmt.Errorf(...) }. Apply to: GetLabels, scanIssues helper, and any other row iteration.","acceptance_criteria":"All row iterations check rows.Err(); partial results never returned silently on error","status":"open","priority":2,"issue_type":"bug","created_at":"2025-10-14T00:38:03.875936-07:00","updated_at":"2025-10-15T11:52:52.2072-07:00","closed_at":"2025-10-14T02:28:44.707577-07:00"}
{"id":"vc-34","title":"Standardize error wrapping in PostgreSQL backend","description":"Error handling inconsistent across PostgreSQL backend. Some places properly wrap errors with fmt.Errorf and %w, others return raw errors. Example: GetLabels postgres.go:878 returns unwrapped 'err' from Scan. Makes debugging harder - can't trace error origin.","design":"Audit all error returns in postgres.go. Ensure every error is wrapped with context using fmt.Errorf with %w verb. Pattern: return nil, fmt.Errorf('operation failed: %w', err).","acceptance_criteria":"All errors in postgres.go properly wrapped with context; error messages include operation that failed","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T00:38:21.410672-07:00","updated_at":"2025-10-15T11:52:52.20736-07:00"}
{"id":"vc-35","title":"Use parameterized LIMIT in GetEvents PostgreSQL query","description":"GetEvents at postgres.go:944 builds LIMIT clause using fmt.Sprintf string concatenation instead of query parameters. While safe (limit is int), this is inconsistent with rest of codebase which uses parameterized queries. Better practice to use placeholders.","design":"Change implementation to use query parameter. If limit \u003e 0, append 'LIMIT $2' to query string and pass limit as second parameter to pool.Query(). Requires adjusting parameter index.","acceptance_criteria":"GetEvents uses parameterized LIMIT clause instead of string concatenation","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T00:38:38.293188-07:00","updated_at":"2025-10-15T11:52:52.207526-07:00"}
{"id":"vc-36","title":"Use consistent timestamp source in AddComment","description":"AddComment at postgres.go:930-932 uses NOW() SQL function for updated_at timestamp. This is evaluated at query execution time, not transaction start. Other functions use time.Now() in Go code for consistency. Mixing sources could cause timestamp ordering issues.","design":"Change 'UPDATE issues SET updated_at = NOW()' to 'UPDATE issues SET updated_at = $N' and pass time.Now() as parameter. Captures timestamp at same moment as event creation. Consistent with rest of codebase.","acceptance_criteria":"AddComment uses Go time.Now() instead of SQL NOW(); timestamps consistent across transaction","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T00:38:54.553625-07:00","updated_at":"2025-10-15T11:52:52.207693-07:00"}
{"id":"vc-37","title":"Use pgx error codes instead of string matching for FK violations in AddDependency","description":"The AddDependency function in postgres.go currently uses brittle string matching to detect foreign key violations (lines 519-527). It uses strings.Contains() to check error messages, which can break if PostgreSQL changes error message format across versions or locales.\n\nCurrent implementation:\nif strings.Contains(err.Error(), \"foreign key constraint\") || strings.Contains(err.Error(), \"violates foreign key\") {\n    if strings.Contains(err.Error(), \"issue_id\") {\n        return fmt.Errorf(\"issue %s not found\", dep.IssueID)\n    }\n    if strings.Contains(err.Error(), \"depends_on_id\") {\n        return fmt.Errorf(\"dependency target %s not found\", dep.DependsOnID)\n    }\n    return fmt.Errorf(\"one or both issues not found\")\n}\n\nThis approach is fragile and not recommended for production code.","design":"Use pgx's error type system to properly detect and handle foreign key violations:\n\n1. Import github.com/jackc/pgx/v5/pgconn\n2. Use errors.As() to check if error is *pgconn.PgError\n3. Check pgErr.Code == \"23503\" (PostgreSQL FK violation error code)\n4. Use pgErr.ConstraintName to determine which FK was violated\n5. Return appropriate error messages based on constraint name\n\nExample:\nvar pgErr *pgconn.PgError\nif errors.As(err, \u0026pgErr) \u0026\u0026 pgErr.Code == \"23503\" {\n    // FK violation - check which constraint\n    if strings.Contains(pgErr.ConstraintName, \"issue_id\") {\n        return fmt.Errorf(\"issue %s not found\", dep.IssueID)\n    }\n    if strings.Contains(pgErr.ConstraintName, \"depends_on\") {\n        return fmt.Errorf(\"dependency target %s not found\", dep.DependsOnID)\n    }\n    return fmt.Errorf(\"one or both issues not found\")\n}\n\nThis uses PostgreSQL's standard error codes which are stable across versions.","acceptance_criteria":"- Import pgconn package\n- Replace string matching with pgErr.Code == \"23503\" check\n- Use pgErr.ConstraintName instead of matching error message text\n- Code compiles successfully\n- Error handling properly identifies which issue doesn't exist","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T00:55:22.478055-07:00","updated_at":"2025-10-15T11:52:52.208091-07:00","closed_at":"2025-10-14T01:02:46.978803-07:00"}
{"id":"vc-38","title":"Add empty ID validation in AddDependency","description":"The AddDependency function in postgres.go does not validate that dep.IssueID and dep.DependsOnID are non-empty before attempting database operations. While the foreign key constraint will catch missing issues, empty strings should be rejected early with a clear error message.\n\nCurrent code only checks for self-dependency:\nif dep.IssueID == dep.DependsOnID {\n    return fmt.Errorf(\"issue cannot depend on itself\")\n}\n\nBut does not check for empty strings, which would fail later with a less clear FK violation error.","design":"Add validation at the start of AddDependency (after self-dependency check):\n\nif dep.IssueID == \"\" || dep.DependsOnID == \"\" {\n    return fmt.Errorf(\"issue IDs cannot be empty\")\n}\n\nThis provides early validation with a clear error message before any database operations are attempted.","acceptance_criteria":"- Add empty string validation for both IssueID and DependsOnID\n- Return clear error message if either is empty\n- Validation occurs before any database operations\n- Code compiles successfully","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-14T00:59:01.996936-07:00","updated_at":"2025-10-15T11:52:52.208427-07:00","closed_at":"2025-10-14T02:28:30.441616-07:00"}
{"id":"vc-39","title":"Add parameter limit protection in DetectCycles bulk query","description":"The DetectCycles function in postgres.go uses a bulk WHERE IN query to fetch all issues involved in cycles (lines 846-863). While this is a huge performance improvement over N+1 queries, it could hit PostgreSQL's parameter limit in pathological cases.\n\nPostgreSQL has a limit of 65535 parameters per query. If there are extremely large cycles or many cycles involving thousands of unique issues, the bulk query could fail.\n\nCurrent code:\nparams := make([]interface{}, len(issueIDList))\nplaceholders := make([]string, len(issueIDList))\nfor i, id := range issueIDList {\n    params[i] = id\n    placeholders[i] = fmt.Sprintf(\"$%d\", i+1)\n}\n\nbulkQuery := fmt.Sprintf(...)\nissueRows, err := s.pool.Query(ctx, bulkQuery, params...)\n\nThis could theoretically exceed parameter limits in extreme cases.","design":"Add batching logic to handle large numbers of issue IDs:\n\n1. Define a reasonable batch size (e.g., 1000 issues per query)\n2. If len(issueIDList) \u003c= batchSize, use current single-query approach\n3. If len(issueIDList) \u003e batchSize, split into batches:\n   - Process batches of up to 1000 IDs each\n   - Merge results into the issueMap\n4. Continue with existing cycle assembly logic\n\nExample:\nconst maxBatchSize = 1000\nissueMap := make(map[string]*types.Issue)\n\nfor i := 0; i \u003c len(issueIDList); i += maxBatchSize {\n    end := i + maxBatchSize\n    if end \u003e len(issueIDList) {\n        end = len(issueIDList)\n    }\n    batch := issueIDList[i:end]\n    \n    // Build and execute query for this batch\n    // Merge results into issueMap\n}\n\nThis ensures we never exceed PostgreSQL's parameter limits.","acceptance_criteria":"- Add batch size constant (1000 issues)\n- Implement batching logic when issue count exceeds limit\n- Single query optimization still used for small result sets\n- All issues fetched and merged correctly\n- Code compiles successfully","status":"open","priority":2,"issue_type":"bug","created_at":"2025-10-14T00:59:15.75732-07:00","updated_at":"2025-10-15T11:52:52.208858-07:00","closed_at":"2025-10-14T02:28:51.518512-07:00"}
{"id":"vc-4","title":"Git Operations Integration","description":"Complete the loop from code changes to mergeable PR. Enables branch creation, commits with proper messages, and PR preparation. Should-have for true 'Engineer-in-a-Box' functionality.","design":"After quality gates pass, automatically: 1) Create feature branch (if not exists), 2) Stage and commit changes with descriptive message, 3) Push to remote, 4) Optionally create PR or prepare for human review. Integrate with issue closer to link commits to issues.","acceptance_criteria":"- Git branch creation/detection\n- Automatic staging of changes\n- Commit message generation (linked to issues)\n- Push to remote support\n- PR creation (via gh CLI or manual prep)\n- Integration with issue workflow\n- Rollback/cleanup on failures","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-13T21:05:19.449252-07:00","updated_at":"2025-10-15T11:52:52.209088-07:00","dependencies":[{"issue_id":"vc-4","depends_on_id":"vc-2","type":"blocks","created_at":"2025-10-13T21:05:19.450149-07:00","created_by":"import"}]}
{"id":"vc-40","title":"Handle missing issues gracefully in DetectCycles","description":"The DetectCycles function in postgres.go silently skips issues that are in the cycle path but cannot be fetched from the database (lines 870-874). This could hide data integrity issues.\n\nCurrent code:\nfor _, issueID := range cp.issueIDs {\n    issue, ok := issueMap[issueID]\n    if !ok {\n        continue  // Silently skip missing issues\n    }\n    cycleIssues = append(cycleIssues, issue)\n}\n\nIf an issue appears in a dependency cycle but doesn't exist in the issues table, this is a data integrity problem that should be logged or reported, not silently ignored.","design":"Add logging or error reporting for missing issues in cycles:\n\nOption 1 (Logging):\nfor _, issueID := range cp.issueIDs {\n    issue, ok := issueMap[issueID]\n    if !ok {\n        // Log data integrity issue\n        s.logger.Warn(\"issue in cycle path not found in database\", \"issue_id\", issueID, \"path\", cp.path)\n        continue\n    }\n    cycleIssues = append(cycleIssues, issue)\n}\n\nOption 2 (Error return):\nReturn an error if any issue in a cycle path is missing, indicating data corruption.\n\nRecommendation: Option 1 (logging) is better - we still want to detect and report other cycles even if one has data integrity issues.","acceptance_criteria":"- Add logging for missing issues in cycle paths\n- Log includes issue ID and full cycle path for debugging\n- Function continues processing other cycles\n- Does not break existing functionality\n- Code compiles successfully","status":"open","priority":3,"issue_type":"bug","created_at":"2025-10-14T00:59:27.400123-07:00","updated_at":"2025-10-15T11:52:52.209278-07:00","closed_at":"2025-10-14T02:29:06.340837-07:00"}
{"id":"vc-41","title":"Use deterministic ordering in DetectCycles for consistent results","description":"The DetectCycles function in postgres.go builds issueIDList from a map using range iteration (lines 836-839), which has non-deterministic order in Go:\n\nissueIDList := make([]string, 0, len(uniqueIssueIDs))\nfor id := range uniqueIssueIDs {\n    issueIDList = append(issueIDList, id)\n}\n\nWhile this doesn't affect correctness, it means the SQL query parameters and results can appear in different orders across runs, making debugging harder and test results non-deterministic.","design":"Sort the issue ID list before building the query:\n\nissueIDList := make([]string, 0, len(uniqueIssueIDs))\nfor id := range uniqueIssueIDs {\n    issueIDList = append(issueIDList, id)\n}\nsort.Strings(issueIDList)  // Add deterministic ordering\n\nThis ensures:\n1. SQL queries are consistent across runs\n2. Test results are deterministic\n3. Debugging is easier (logs show same order)\n4. No performance impact (sorting small lists is fast)\n\nNote: The cycles themselves are already ordered by the SQL query's ORDER BY clause, so this only affects the intermediate issue fetching.","acceptance_criteria":"- Import sort package\n- Sort issueIDList after building from map\n- SQL queries use consistent parameter order\n- Tests produce deterministic results\n- Code compiles successfully","status":"open","priority":3,"issue_type":"bug","created_at":"2025-10-14T00:59:38.807672-07:00","updated_at":"2025-10-15T11:52:52.209484-07:00","closed_at":"2025-10-14T02:28:59.227016-07:00"}
{"id":"vc-42","title":"Empty Backend string should default to sqlite in NewStorage","description":"In storage.go NewStorage() function, when cfg.Backend is an empty string, the switch statement falls through to the default case and returns an error: 'unsupported backend:  (must be 'sqlite' or 'postgres')'.\n\nThis is inconsistent with DefaultConfig() which sets Backend='sqlite' as the default. Users might pass a Config with Backend=\"\" expecting it to use the default.\n\nCurrent behavior at lines 111-173:\nswitch cfg.Backend {\ncase \"sqlite\": ...\ncase \"postgres\": ...\ndefault:\n    return nil, fmt.Errorf(\"unsupported backend: %s...\", cfg.Backend)\n}\n\nWhen Backend=\"\", this returns an error instead of defaulting to sqlite.","design":"Add explicit handling for empty Backend string before the switch statement:\n\nif cfg.Backend == \"\" {\n    cfg.Backend = \"sqlite\"\n}\n\nswitch cfg.Backend {\ncase \"sqlite\": ...\ncase \"postgres\": ...\ndefault: ...\n}\n\nThis matches the behavior of DefaultConfig() which uses sqlite as the default backend.","acceptance_criteria":"- Empty Backend string defaults to \"sqlite\"\n- Error message still shown for invalid backend names\n- DefaultConfig() and NewStorage() have consistent default behavior\n- Code compiles successfully","status":"open","priority":2,"issue_type":"bug","created_at":"2025-10-14T01:22:31.28857-07:00","updated_at":"2025-10-15T11:52:52.209665-07:00","closed_at":"2025-10-14T12:49:22.523728-07:00"}
{"id":"vc-43","title":"Remove duplicate default logic in NewStorage postgres path","description":"In storage.go NewStorage() function, PostgreSQL connection pool defaults are duplicated in two places:\n\n1. storage.DefaultConfig() sets defaults at lines 87-101\n2. NewStorage() re-applies defaults at lines 149-167\n\nThis creates maintenance burden - if postgres.DefaultConfig() changes, NewStorage() must also be updated. The defaults could drift out of sync.\n\nCurrent code:\n// Build postgres config\npgCfg := \u0026postgres.Config{\n    Host: cfg.Host,\n    MaxConns: cfg.MaxConns,  // Might be 0\n    ...\n}\n\n// Apply defaults if not set\nif pgCfg.MaxConns == 0 {\n    pgCfg.MaxConns = 25  // Hardcoded duplicate\n}\n\nThis duplicates the default value of 25 that's already in postgres.DefaultConfig().","design":"Use postgres.DefaultConfig() as the base and merge user-provided values:\n\n// Start with postgres defaults\npgCfg := postgres.DefaultConfig()\n\n// Override with user-provided values\npgCfg.Host = cfg.Host\npgCfg.Port = cfg.Port\npgCfg.Database = cfg.Database\npgCfg.User = cfg.User\npgCfg.Password = cfg.Password\n\n// Only override pool settings if explicitly set (non-zero)\nif cfg.SSLMode != \"\" {\n    pgCfg.SSLMode = cfg.SSLMode\n}\nif cfg.MaxConns != 0 {\n    pgCfg.MaxConns = cfg.MaxConns\n}\n// ... etc\n\nThis ensures postgres.DefaultConfig() is the single source of truth for defaults.","acceptance_criteria":"- Remove duplicate default values from NewStorage()\n- Use postgres.DefaultConfig() as base for building pgCfg\n- User-provided values override defaults\n- Defaults only exist in one place (postgres.DefaultConfig)\n- Code compiles successfully","status":"open","priority":2,"issue_type":"bug","created_at":"2025-10-14T01:22:42.858746-07:00","updated_at":"2025-10-15T11:52:52.209944-07:00","closed_at":"2025-10-14T11:16:50.392733-07:00"}
{"id":"vc-44","title":"Document SQLite/PostgreSQL context parameter inconsistency","description":"The SQLite and PostgreSQL backend constructors have inconsistent signatures:\n\n- sqlite.New(path string) (*SQLiteStorage, error)  // No context\n- postgres.New(ctx context.Context, cfg *Config) (*PostgresStorage, error)  // Takes context\n\nThis inconsistency is visible in NewStorage() at lines 117 vs 169:\nreturn sqlite.New(cfg.Path)      // No context passed\nreturn postgres.New(ctx, pgCfg)  // Context passed\n\nThis is existing technical debt in the backend implementations, not something created by vc-21. However, it should be documented as a known issue.\n\nImpact:\n- Inconsistent API design between backends\n- SQLite can't respect context cancellation during initialization\n- Makes it harder to swap backends (different function signatures)","design":"Add a comment in storage.go documenting this inconsistency:\n\n// NewStorage creates a new storage backend based on configuration\n// Note: SQLite backend does not accept a context parameter, only PostgreSQL does.\n// This is a known API inconsistency between the backends.\nfunc NewStorage(ctx context.Context, cfg *Config) (Storage, error) {\n    ...\n}\n\nOptionally create a follow-up issue to fix the backend implementations:\n- Update sqlite.New() to accept context\n- Use context for initialization operations\n- Make both backends consistent","acceptance_criteria":"- Comment added documenting the context parameter inconsistency\n- Explains why sqlite.New() doesn't receive ctx\n- Optional: Create follow-up issue to fix sqlite.New() signature","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-14T01:22:55.764305-07:00","updated_at":"2025-10-15T11:52:52.210137-07:00","closed_at":"2025-10-14T11:16:22.70806-07:00"}
{"id":"vc-45","title":"DefaultConfig should only populate fields for selected backend","description":"In storage.go, DefaultConfig() populates fields for both SQLite and PostgreSQL backends, even though only one will be used:\n\nreturn \u0026Config{\n    Backend:         \"sqlite\",\n    Path:            \".beads/vc.db\",\n    Host:            \"localhost\",    // PostgreSQL fields\n    Port:            5432,\n    Database:        \"vc\",\n    User:            \"vc\",\n    MaxConns:        25,\n    // ... etc\n}\n\nWhen Backend=\"sqlite\", the PostgreSQL fields (Host, Port, Database, User, MaxConns, etc.) are unused but still allocated.\n\nIssues:\n- Wastes memory for unused fields\n- Confusing: why does a SQLite config have \"User\" and \"Port\" set?\n- Makes config serialization/display messy (shows irrelevant fields)\n\nThis is a minor issue but reduces clarity.","design":"Option 1 (simpler): Keep current behavior, add documentation\n- Document that Config contains fields for all backends\n- Note that only relevant fields are used based on Backend value\n\nOption 2 (cleaner): Make backend-specific defaults\n- Create DefaultSQLiteConfig() and DefaultPostgresConfig() functions\n- DefaultConfig() just returns DefaultSQLiteConfig()\n- Users can call the specific one they need\n\nRecommendation: Option 1 for now (just document), Option 2 if we add more backends.","acceptance_criteria":"- Add comment explaining Config contains fields for all backends\n- Document that irrelevant fields are ignored based on Backend value\n- Optional: Consider backend-specific config functions for future","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T01:23:16.765641-07:00","updated_at":"2025-10-15T11:52:52.210319-07:00"}
{"id":"vc-46","title":"Add validation that Backend matches populated config fields","description":"In storage.go, there's no validation that the Backend field matches the populated configuration fields. This allows invalid configurations:\n\nExample 1: Backend=\"sqlite\" but PostgreSQL fields populated\ncfg := \u0026Config{\n    Backend: \"sqlite\",\n    Path: \".beads/vc.db\",\n    Host: \"localhost\",  // Irrelevant for SQLite\n    Port: 5432,\n    Database: \"vc\",\n}\n\nExample 2: Backend=\"postgres\" but Path populated\ncfg := \u0026Config{\n    Backend: \"postgres\",\n    Path: \".beads/vc.db\",  // Irrelevant for PostgreSQL\n    Host: \"localhost\",\n    Port: 5432,\n}\n\nNewStorage() will work but silently ignore the wrong fields. This makes debugging configuration issues harder - users won't know why their Host setting isn't working when they meant to use postgres but accidentally set Backend=\"sqlite\".","design":"Add validation in NewStorage() before the switch statement:\n\n// Validate config consistency\nif cfg.Backend == \"sqlite\" {\n    if cfg.Host \\!= \"\" || cfg.Port \\!= 0 {\n        // Option 1: Warning (log)\n        log.Warn(\"PostgreSQL fields set but Backend is sqlite, ignoring\")\n        \n        // Option 2: Error (strict)\n        return nil, fmt.Errorf(\"Backend is sqlite but PostgreSQL fields are set\")\n    }\n}\n\nRecommendation: Option 1 (warning) for now, since Config is a flat struct. If we add more backends, consider splitting into backend-specific config types.","acceptance_criteria":"- Add validation checking Backend matches populated fields\n- Either log warning or return error for mismatches\n- Document which fields are relevant for which backend\n- Code compiles successfully","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T01:23:31.143189-07:00","updated_at":"2025-10-15T11:52:52.211925-07:00"}
{"id":"vc-47","title":"Normalize Backend string to lowercase before comparison in NewStorage","description":"In storage.go NewStorage() function, the Backend field is compared case-sensitively:\n\nswitch cfg.Backend {\ncase \"sqlite\":\n    ...\ncase \"postgres\":\n    ...\n}\n\nThis means \"SQLite\", \"SQLITE\", \"Postgres\", \"PostgreSQL\", etc. would all fail with 'unsupported backend' error.\n\nWhile the documentation says Backend should be \"sqlite\" or \"postgres\", users might naturally type \"SQLite\" or \"PostgreSQL\". Case-insensitive matching would be more user-friendly.\n\nCurrent behavior:\n- \"sqlite\" ✓ works\n- \"SQLite\" ✗ error\n- \"postgres\" ✓ works  \n- \"PostgreSQL\" ✗ error\n\nExpected behavior: All should work.","design":"Normalize Backend to lowercase before the switch:\n\nfunc NewStorage(ctx context.Context, cfg *Config) (Storage, error) {\n    if cfg == nil {\n        cfg = DefaultConfig()\n    }\n    \n    // Normalize backend name to lowercase\n    backend := strings.ToLower(cfg.Backend)\n    \n    // Validate backend type\n    switch backend {\n    case \"sqlite\":\n        ...\n    case \"postgres\":\n        ...\n    default:\n        return nil, fmt.Errorf(\"unsupported backend: %s (must be 'sqlite' or 'postgres')\", cfg.Backend)\n    }\n}\n\nNote: Keep original cfg.Backend in error message so user sees what they actually typed.","acceptance_criteria":"- Add strings.ToLower() before switch statement\n- All case variations of sqlite/postgres work\n- Error message shows original (non-normalized) Backend value\n- Code compiles successfully","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T01:23:42.844679-07:00","updated_at":"2025-10-15T11:52:52.215838-07:00"}
{"id":"vc-48","title":"Fix race condition in UpdateExecutionState using atomic UPDATE","description":"UpdateExecutionState in postgres/execution_state.go:129-147 has a race condition. It reads current state with GetExecutionState, validates transition, then updates. Between read and update, another transaction could modify the state.\n\nCurrent flow:\n1. GetExecutionState (read)\n2. Validate transition\n3. UPDATE (write)\n\nThis is a check-then-act race condition.","design":"Replace the read-validate-update pattern with a single atomic UPDATE that includes the validation:\n\n```go\nquery := `\n    UPDATE issue_execution_state\n    SET state = $1, updated_at = $2\n    WHERE issue_id = $3 AND state = $4\n`\nresult, err := s.pool.Exec(ctx, query, newState, time.Now(), issueID, expectedCurrentState)\nif result.RowsAffected() == 0 {\n    // Either issue not found OR state changed (concurrent modification)\n    // Need to distinguish these cases\n}\n```\n\nAlternative: Use SELECT FOR UPDATE in a transaction to lock the row.","acceptance_criteria":"- UpdateExecutionState prevents concurrent state modifications\n- Invalid transitions still return appropriate errors\n- No performance regression\n- Tests verify concurrent update behavior","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T03:35:01.448793-07:00","updated_at":"2025-10-15T11:52:52.21678-07:00","closed_at":"2025-10-14T03:37:25.948754-07:00"}
{"id":"vc-49","title":"Remove redundant claim check in ClaimIssue","description":"ClaimIssue in postgres/execution_state.go:25-33 performs a redundant SELECT to check if issue is already claimed before the INSERT. The INSERT itself will fail with a unique constraint violation (error 23505) if the issue is already claimed, which we already handle at lines 66-70.\n\nThe redundant check adds an extra database roundtrip without providing additional safety.","design":"Remove lines 25-33:\n```go\n// Check if issue is already claimed\nvar existingExecutor string\nerr = tx.QueryRow(ctx, \"SELECT executor_instance_id FROM issue_execution_state WHERE issue_id = $1\", issueID).Scan(\u0026existingExecutor)\nif err != nil \u0026\u0026 err != pgx.ErrNoRows {\n    return fmt.Errorf(\"failed to check execution state: %w\", err)\n}\nif err == nil {\n    return fmt.Errorf(\"issue %s is already claimed by another executor\", issueID)\n}\n```\n\nThe existing constraint check at lines 66-70 is sufficient and atomic.","acceptance_criteria":"- ClaimIssue still prevents double-claiming\n- One fewer database roundtrip per claim\n- All existing tests still pass\n- Error message for already-claimed issues remains clear","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-14T03:35:10.838747-07:00","updated_at":"2025-10-15T11:52:52.217111-07:00","closed_at":"2025-10-14T11:16:01.574132-07:00"}
{"id":"vc-5","title":"Beads Integration and Executor Tables","description":"Foundation work: Extend Beads with VC-specific fields and executor tables. This is the base layer that all other phases depend on.","design":"Add: 1) discovered-from dependency type (already in Beads), 2) design/acceptance_criteria/notes fields (already in Beads), 3) executor_instances table for tracking worker instances, 4) issue_execution_state table for checkpointing/resumption, 5) Ensure PostgreSQL backend works alongside SQLite","acceptance_criteria":"- discovered-from dependency type verified working\n- design, acceptance_criteria, notes fields verified working\n- executor_instances table created and schema documented\n- issue_execution_state table created with checkpoint support\n- PostgreSQL backend tested (while keeping SQLite as default)\n- Migration/initialization scripts created\n- All beads tests passing with new tables","notes":"Progress: 2 of 6 child tasks completed (vc-10, vc-11). executor_instances table fully implemented with type-safe enum and validation. Next: vc-12 (issue_execution_state table).","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-13T21:05:19.449345-07:00","updated_at":"2025-10-15T19:47:32.618351-07:00","closed_at":"2025-10-14T13:10:24.885139-07:00"}
{"id":"vc-50","title":"Make GROUP BY explicit in GetBlockedIssues","description":"GetBlockedIssues in postgres/ready.go:93 uses 'GROUP BY i.id' which works because i.id is a primary key, but some PostgreSQL configurations require all non-aggregated SELECT columns to be explicitly listed in GROUP BY.\n\nThe schema view (schema.go:92-94) does this correctly by listing all columns.","design":"Replace:\n```go\nGROUP BY i.id\n```\n\nWith:\n```go\nGROUP BY i.id, i.title, i.description, i.design, i.acceptance_criteria, i.notes,\n         i.status, i.priority, i.issue_type, i.assignee, i.estimated_minutes,\n         i.created_at, i.updated_at, i.closed_at\n```\n\nThis matches the pattern used in the blocked_issues view in schema.go.","acceptance_criteria":"- Query works on all PostgreSQL configurations\n- No change in query results\n- Matches schema view pattern\n- All tests pass","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-14T03:35:19.936792-07:00","updated_at":"2025-10-15T11:52:52.217519-07:00","closed_at":"2025-10-14T11:15:40.379072-07:00"}
{"id":"vc-51","title":"Add PostgreSQL integration tests for execution state operations","description":"The PostgreSQL backend execution state and ready work operations lack integration tests. Currently we only have SQLite tests. This creates a gap in test coverage for PostgreSQL-specific behavior:\n\n- JSONB checkpoint data handling\n- array_agg in GetBlockedIssues\n- PostgreSQL-specific error codes (23505)\n- COUNT(*) FILTER syntax\n- EXTRACT(EPOCH) for timestamps\n\nThe SQLite tests verify the business logic, but not the PostgreSQL-specific implementation details.","design":"Create postgres/execution_state_test.go and postgres/ready_test.go that:\n\n1. Use testcontainers or similar to spin up PostgreSQL\n2. Port key tests from sqlite/execution_state_test.go\n3. Add PostgreSQL-specific tests:\n   - JSONB marshaling/unmarshaling edge cases\n   - array_agg with various data\n   - Concurrent ClaimIssue attempts (race testing)\n   - Verify error codes match expectations\n\nFollow the pattern in sqlite tests but adapt for PostgreSQL connection handling.","acceptance_criteria":"- PostgreSQL-specific tests exist and pass\n- Tests cover JSONB, array_agg, error codes\n- Tests use real PostgreSQL (not mocks)\n- CI can run tests (either with testcontainers or postgres service)\n- Test coverage metrics show adequate coverage","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T03:35:31.600395-07:00","updated_at":"2025-10-15T11:52:52.217707-07:00"}
{"id":"vc-52","title":"Add defensive handling for array_agg null in GetBlockedIssues","description":"GetBlockedIssues in postgres/ready.go:86 uses array_agg(d.depends_on_id) which could theoretically return {NULL} in edge cases. While the JOINs should prevent this, defensive programming suggests handling it.\n\nCurrently we scan directly into []string at line 107. If array_agg somehow returns {NULL}, we'd get a slice with one empty/null element.","design":"Add validation after scanning blockerIDs:\n\n```go\nissue.BlockedBy = blockerIDs\n\n// Filter out any null/empty blocker IDs (defensive)\nif len(blockerIDs) == 1 \u0026\u0026 blockerIDs[0] == \"\" {\n    issue.BlockedBy = []string{}\n}\n```\n\nOr use COALESCE in the query:\n```sql\nCOALESCE(array_agg(d.depends_on_id), ARRAY[]::text[]) as blocker_ids\n```\n\nThe query approach is cleaner.","acceptance_criteria":"- GetBlockedIssues handles edge cases gracefully\n- No null/empty strings in BlockedBy arrays\n- Existing behavior unchanged for normal cases\n- Add test case for edge condition if possible","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T03:35:44.288451-07:00","updated_at":"2025-10-15T11:52:52.218001-07:00"}
{"id":"vc-53","title":"Fix double ReleaseIssue call in executor","description":"Bug: executeIssue() was calling ReleaseIssue() twice - once in the error path (via releaseIssueWithError) and again unconditionally at the end. This caused 'execution state not found' errors.\n\nFix: Moved ReleaseIssue() call inside the success branch only, since releaseIssueWithError() already handles the error path.","acceptance_criteria":"\n- ReleaseIssue only called once per execution\n- No 'execution state not found' errors\n- Error path properly releases via releaseIssueWithError\n- Success path properly releases before return\n","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-14T14:12:35.515401-07:00","updated_at":"2025-10-15T11:52:52.218179-07:00","closed_at":"2025-10-14T14:12:43.113528-07:00"}
{"id":"vc-54","title":"Fix race condition in agent output capture","description":"The agent output capture goroutines have a race condition where console printing happens outside the mutex lock.\n\nLocation: internal/executor/agent.go:207-208 and 229-230\n\nIssue:\n- Line 207: fmt.Println(line) happens outside mutex\n- Line 229: fmt.Fprintln(os.Stderr, line) happens outside mutex\n- The mutex-protected append and the console print can interleave\n\nThis could cause output to appear out of order on the console vs. what's captured in memory.","design":"Move the console printing inside the mutex:\n\n// Capture stdout\ngo func() {\n    defer wg.Done()\n    scanner := bufio.NewScanner(a.stdout)\n    for scanner.Scan() {\n        line := scanner.Text()\n        a.mu.Lock()\n        \n        if len(a.result.Output) \u003c maxOutputLines {\n            a.result.Output = append(a.result.Output, line)\n            // Print inside mutex to ensure ordering\n            fmt.Println(line)\n        } else if len(a.result.Output) == maxOutputLines {\n            a.result.Output = append(a.result.Output, \"[... truncated ...]\")\n        }\n        \n        a.mu.Unlock()\n    }\n}()\n\nSame pattern for stderr capture.","acceptance_criteria":"\n- Console output order matches captured output order\n- No race conditions detected by go test -race\n- Output capture still works correctly\n","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-14T14:14:56.593386-07:00","updated_at":"2025-10-15T11:52:52.21875-07:00","closed_at":"2025-10-14T14:19:31.003811-07:00"}
{"id":"vc-55","title":"Improve agent process cleanup and verification","description":"When agent execution times out, we call Kill() but don't verify the process actually died.\n\nLocation: internal/executor/agent.go:140\n\nIssue:\n- Kill() may fail silently\n- Process could become a zombie\n- No verification that kill succeeded\n\nThis could lead to orphaned processes accumulating over time.","design":"After calling Kill(), wait briefly and verify the process is dead:\n\na.Kill()\n// Give process time to die\ntime.Sleep(100 * time.Millisecond)\nif a.cmd.Process != nil {\n    // Check if process still exists\n    if err := a.cmd.Process.Signal(syscall.Signal(0)); err == nil {\n        // Process still alive - escalate to SIGKILL on Unix\n        a.cmd.Process.Signal(syscall.SIGKILL)\n    }\n}\n\nPlatform-specific considerations:\n- Unix: Can use SIGKILL if SIGTERM fails\n- Windows: Process.Kill() is already forceful\n\nConsider adding a ProcessManager to track spawned agents for cleanup on executor shutdown.","acceptance_criteria":"\n- Kill() failures are detected and logged\n- Zombie processes are prevented\n- Process cleanup verified before returning\n- Consider adding agent process registry\n","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T14:15:10.60387-07:00","updated_at":"2025-10-15T11:52:52.218921-07:00"}
{"id":"vc-56","title":"Add logging for claim race conditions","description":"When ClaimIssue fails due to race condition (another executor claimed it first), we silently return nil. This makes it hard to debug multi-executor scenarios.\n\nLocation: internal/executor/executor.go:218-221\n\nCurrent behavior:\nif err := e.store.ClaimIssue(ctx, issue.ID, e.instanceID); err != nil {\n    return nil  // Silent ignore\n}\n\nThis is correct behavior (race conditions are expected), but we should log for observability.","design":"Add debug-level logging when claim fails:\n\nif err := e.store.ClaimIssue(ctx, issue.ID, e.instanceID); err != nil {\n    fmt.Fprintf(os.Stderr, \"debug: issue %s already claimed: %v\\n\", issue.ID, err)\n    return nil\n}\n\nLater, when adding structured logging:\n- Use debug level (not error)\n- Include issue ID and executor instance ID\n- Track claim race metrics","acceptance_criteria":"\n- Claim failures logged at debug level\n- Log includes issue ID and reason\n- Does not spam logs under normal operation\n- Helps debug multi-executor scenarios\n","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T14:15:24.430065-07:00","updated_at":"2025-10-15T11:52:52.219119-07:00"}
{"id":"vc-57","title":"Add execution state transitions to executor workflow","description":"Currently executor only sets execution state to 'executing' once. Need to update state as issue progresses through phases: claimed -\u003e assessing -\u003e executing -\u003e analyzing -\u003e gates -\u003e completed. This makes debugging much easier - you can see exactly where an issue is stuck.","design":"Update executeIssue() in internal/executor/executor.go to call UpdateExecutionState() at each phase transition:\n- Before AI assessment: ExecutionStateAssessing\n- Before spawning agent: ExecutionStateExecuting  \n- After agent completes: ExecutionStateAnalyzing\n- After AI analysis (if quality gates enabled): ExecutionStateGates\n- After successful completion: ExecutionStateCompleted\n\nAlso update error paths to set appropriate states.","acceptance_criteria":"- State transitions logged at each phase\n- Can query database to see which phase an issue is in\n- Error handling preserves state information","notes":"Implementation complete:\n- Added ExecutionStateAssessing before AI assessment (line 256)\n- Added ExecutionStateExecuting before spawning agent (line 287)  \n- Added ExecutionStateAnalyzing after agent completes, before AI analysis (line 316)\n- Added ExecutionStateCompleted on successful completion (line 398)\n\nState transition flow:\n- With AI supervision: assessing -\u003e executing -\u003e analyzing -\u003e completed\n- Without AI supervision: executing -\u003e completed\n\nExecutionStateGates will be used when vc-8 (Quality Gates) is implemented.\n\nAll state updates use warning-level logging on failure so they don't break execution.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-14T14:54:07.487003-07:00","updated_at":"2025-10-15T11:52:52.221157-07:00","closed_at":"2025-10-14T15:04:04.296161-07:00"}
{"id":"vc-58","title":"Add resilient JSON parser for AI responses","description":"AI responses sometimes include markdown code fences, explanatory text, or other non-JSON content. Current json.Unmarshal() fails hard on malformed responses. Port the resilient JSON parser from vibecoder (src/utils/json-parser.ts, safe-json-parser.ts) to handle common AI response patterns.","design":"Create internal/ai/json_parser.go with:\n- Strip markdown code fences (backticks)\n- Extract JSON from mixed text responses\n- Handle common AI quirks (trailing commas, comments, etc.)\n- Fallback strategies for partial JSON\n- Log warnings but don't fail on parse errors\n\nReference vibecoder implementation at:\n- src/utils/json-parser-unified.ts\n- src/utils/safe-json-parser.ts\n- test/utils/resilient-json-parser.test.ts","acceptance_criteria":"- Can parse JSON from markdown code blocks\n- Handles common AI response variations\n- Comprehensive test suite\n- Logs parse warnings without failing","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-14T14:54:09.202867-07:00","updated_at":"2025-10-15T11:52:52.221899-07:00","closed_at":"2025-10-14T18:07:43.172371-07:00"}
{"id":"vc-59","title":"Add timeout and retry logic for AI API calls","description":"AI API calls can hang or fail due to transient network issues. Need timeout context and retry logic with exponential backoff. Vibecoder had robust retry through Temporal - we need lightweight Go equivalent.","design":"In internal/ai/supervisor.go:\n1. Add context timeout (60s) to API calls\n2. Implement retry with exponential backoff:\n   - Max 3 retries\n   - Backoff: 1s, 2s, 4s\n   - Only retry on transient errors (network, 5xx, rate limits)\n   - Don't retry on 4xx client errors\n3. Add circuit breaker pattern to prevent cascading failures\n4. Make retry config tunable\n\nReference vibecoder patterns in:\n- src/executor/issue-workflow-executor.ts\n- Temporal retry policies (though we're not using Temporal)","acceptance_criteria":"- API calls timeout after 60s\n- Transient failures automatically retried\n- Circuit breaker prevents cascading failures\n- Retry metrics logged","notes":"Implemented timeout and retry logic with exponential backoff (1s, 2s, 4s). API calls now timeout after 60s. Transient errors (5xx, timeouts, network errors, rate limits) are automatically retried. Retry metrics are logged.\n\nStill TODO: Circuit breaker pattern (follow-up issue created). Current implementation provides substantial resilience - retries handle transient failures, non-retriable errors (4xx) fail fast.\n\nChanges in internal/ai/supervisor.go:\n- Added RetryConfig struct with tunable parameters\n- Added retryWithBackoff() with exponential backoff\n- Added isRetriableError() to classify errors\n- Wrapped both API calls with retry logic\n\nTests passing. Ready for review.","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-14T14:54:10.548504-07:00","updated_at":"2025-10-15T11:52:52.223635-07:00"}
{"id":"vc-6","title":"Issue Processor Event Loop","description":"Build the event loop that claims and executes issues via coding agents. No AI supervision yet - just mechanical claiming, spawning agents, and updating status. The core execution engine.","design":"Port IssueWorkflowExecutor pattern: 1) Atomic claiming with PostgreSQL FOR UPDATE SKIP LOCKED, 2) Spawn Cody/Claude Code with -stream-json, 3) Parse agent output and update issue status, 4) Handle epic completion detection, 5) Support pause/resume/abort. Pure orchestration layer with no AI decision-making yet.","acceptance_criteria":"- Event loop running continuously\n- Atomic issue claiming from ready work queue\n- Cody/Claude Code spawning and lifecycle management\n- Agent output parsing and status updates\n- Epic completion detection (all children closed)\n- Pause/resume/abort commands working\n- Basic error handling and retries\n- 'vc execute' command working end-to-end","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-13T21:05:19.449438-07:00","updated_at":"2025-10-15T19:48:22.229347-07:00","closed_at":"2025-10-14T13:52:08.417303-07:00","dependencies":[{"issue_id":"vc-6","depends_on_id":"vc-5","type":"blocks","created_at":"2025-10-13T21:05:19.450237-07:00","created_by":"import"}]}
{"id":"vc-60","title":"Add integration tests for AI supervision","description":"AI supervision code (vc-7) has no tests. Need comprehensive test coverage for assessment, analysis, discovered issue creation, and error handling.","design":"Create internal/ai/supervisor_test.go with:\n1. Mock Anthropic client for deterministic testing\n2. Unit tests:\n   - Assessment prompt building\n   - Analysis prompt building\n   - JSON parsing (happy path + errors)\n   - Priority/type mapping\n   - Discovered issue creation\n3. Integration tests:\n   - Full assessment -\u003e execution -\u003e analysis flow\n   - Fallback behavior when AI fails\n   - Partial failure scenarios (some issues created, then error)\n4. Table-driven tests for edge cases\n\nAlso create internal/executor/executor_test.go for executor AI integration tests.","acceptance_criteria":"- \u003e80% code coverage for internal/ai package\n- Mock client for repeatable tests\n- Tests for error paths and edge cases\n- CI pipeline runs tests automatically","notes":"Test implementation complete:\n\n**What's tested (100% coverage):**\n- buildAssessmentPrompt() - Verified all issue fields included\n- buildAnalysisPrompt() - Tested success/failure scenarios\n- truncateString() - All edge cases covered\n- CreateDiscoveredIssues() - 96.2% coverage:\n  - Single and multiple issue creation\n  - All type mappings (bug/task/feature/chore/epic)\n  - All priority mappings (P0-P3, defaults)\n  - Dependency creation with DepDiscoveredFrom\n  - Partial failure scenarios\n  - Edge cases (unknown types, empty values)\n\n**Test stats:**\n- 8 test functions\n- 21 test cases (using subtests)\n- Overall package coverage: 40.2%\n\n**What's NOT tested (requires API mocking):**\n- AssessIssueState() - Anthropic API call\n- AnalyzeExecutionResult() - Anthropic API call  \n- NewSupervisor() - Constructor\n- logAIUsage() - Simple logging wrapper\n\nThe core business logic is comprehensively tested. API integration tests would require complex mocking of anthropic.Client which is a separate effort (see vc-58, vc-59 for API improvements).\n\nAll tests pass: `go test ./internal/ai/... -v`","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-14T14:54:12.185598-07:00","updated_at":"2025-10-15T11:52:52.225449-07:00","closed_at":"2025-10-14T15:32:34.026482-07:00"}
{"id":"vc-61","title":"Fix variable shadowing in executor AI analysis loop","description":"In executor.go line 356, loop variable 'issue' shadows function parameter. This works but is confusing and could cause bugs during refactoring.","design":"Change line 356 in internal/executor/executor.go from:\n  for _, issue := range analysis.QualityIssues {\nto:\n  for _, qi := range analysis.QualityIssues {\n\tanalysisComment += fmt.Sprintf(\"- %s\\n\", qi)\n\nSimple one-line fix.","acceptance_criteria":"- No variable shadowing warnings\n- Code compiles and tests pass","status":"open","priority":2,"issue_type":"bug","created_at":"2025-10-14T14:54:14.266277-07:00","updated_at":"2025-10-15T11:52:52.225863-07:00"}
{"id":"vc-62","title":"Clarify or fix truncateString() behavior","description":"truncateString() in supervisor.go line 347 takes the LAST N characters, but the name suggests it should take the first N. For agent output we probably want the most recent (end), but the function is misleading.","design":"Two options:\n1. Rename to takeLastNChars() to be explicit\n2. Change implementation to s[:maxLen] if we want the beginning\n\nCurrent usage: truncateString(agentOutput, 2000) for AI analysis\nProbably want the END of agent output (most recent), so option 1 is better.\n\nAlso consider: taking both first 1000 and last 1000 chars to give AI context about what was attempted AND the final result.","acceptance_criteria":"- Function name matches behavior\n- Documentation clarifies head vs tail truncation","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T14:54:15.725024-07:00","updated_at":"2025-10-15T11:52:52.229367-07:00"}
{"id":"vc-63","title":"Document or handle partial failure in CreateDiscoveredIssues","description":"CreateDiscoveredIssues() returns error after creating some issues (line 322). If 3 issues are created successfully and the 4th fails, the 3 already exist but we return an error. This could cause confusion or duplicates on retry.","design":"Options:\n1. Continue on error, collect all failures, return multi-error at end\n2. Implement transaction/rollback (delete already-created issues on failure)\n3. Document the behavior clearly and make retry idempotent\n4. Return (createdIDs, failedIssues, error) for partial success reporting\n\nOption 1 or 3 seems best for Zero Framework Cognition - let AI figure out what to do with partial failures.","acceptance_criteria":"- Behavior is documented\n- Retry logic handles partial failures gracefully\n- Logs clearly indicate which issues were created vs failed","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-14T14:54:17.361793-07:00","updated_at":"2025-10-15T11:52:52.229618-07:00"}
{"id":"vc-64","title":"Optimize duplicate extractSummary calls in executor","description":"executor.go calls extractSummary(result) twice with same input (lines 309 and 340). Minor inefficiency - should reuse the variable.","design":"Move the extractSummary() call earlier (before AI analysis) and reuse:\n  // After agent completes (line ~304)\n  agentOutput := e.extractSummary(result)\n  \n  // Use in AI analysis (line ~309)\n  analysis, err = e.supervisor.AnalyzeExecutionResult(ctx, issue, agentOutput, result.Success)\n  \n  // Reuse for comment (line ~340)\n  if err := e.store.AddComment(ctx, issue.ID, e.instanceID, agentOutput); err != nil {","acceptance_criteria":"- extractSummary() called once per execution\n- Behavior unchanged\n- Tests still pass","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-10-14T14:54:19.463074-07:00","updated_at":"2025-10-15T11:52:52.229768-07:00","closed_at":"2025-10-14T15:57:05.154607-07:00"}
{"id":"vc-65","title":"Replace fmt.Fprintf with structured logging (log/slog)","description":"Current code uses fmt.Fprintf(os.Stderr, ...) for warnings and errors. Should migrate to structured logging with log/slog for better observability and log parsing.","design":"1. Add log/slog setup in executor and AI supervisor packages\n2. Replace fmt.Fprintf calls with slog.Warn/Error with structured fields\n3. Add log levels configuration\n4. Use context-aware logging where applicable\nExample: slog.Warn('AI assessment failed', 'issue', issue.ID, 'error', err)","acceptance_criteria":"All fmt.Fprintf error/warning calls replaced with slog\nLogs include structured fields (issue IDs, error context)\nLog level is configurable","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-14T15:56:33.18569-07:00","updated_at":"2025-10-15T11:52:52.229913-07:00"}
{"id":"vc-66","title":"Define magic numbers as constants","description":"Code has several magic numbers that should be named constants for maintainability. Examples: 2000 (truncate length in supervisor.go:267), 4096 (max tokens in supervisor.go), timeout durations, poll intervals.","design":"1. Create constants file or add to existing config\n2. Replace magic numbers with named constants:\n   - const maxAnalysisOutputLen = 2000\n   - const maxAITokens = 4096\n   - const defaultPollInterval = 5 * time.Second\n   - const defaultHeartbeatPeriod = 30 * time.Second\n3. Document why each value was chosen","acceptance_criteria":"No magic numbers in core logic\nAll constants documented\nTests still pass","status":"open","priority":3,"issue_type":"chore","created_at":"2025-10-14T15:56:45.457392-07:00","updated_at":"2025-10-15T11:52:52.231458-07:00"}
{"id":"vc-67","title":"Add performance metrics and tracing","description":"Add instrumentation for performance monitoring: execution times, AI API latency, issue throughput, success/failure rates. Important for production observability and identifying bottlenecks.","design":"1. Add metrics package (internal/metrics)\n2. Track key metrics:\n   - Issue execution duration (by type, priority)\n   - AI API call latency (assessment, analysis)\n   - Agent spawn/completion times\n   - Success/failure rates\n   - Issues claimed/completed per hour\n3. Expose metrics via:\n   - Prometheus endpoint (/metrics)\n   - Periodic log summaries\n   - In-memory stats accessible via CLI (vc stats)\n4. Add tracing for distributed debugging (optional: OpenTelemetry)","acceptance_criteria":"Key metrics tracked\nMetrics accessible via CLI and/or HTTP\nPerformance regressions detectable\nDocumentation for metrics interpretation","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T15:56:57.866141-07:00","updated_at":"2025-10-15T11:52:52.232074-07:00"}
{"id":"vc-68","title":"Basic REPL loop with readline and command parsing","description":"Implement the core REPL loop with readline support, basic command parsing, and essential commands (exit, quit, help). Foundation for all other REPL features.","design":"Use github.com/chzyer/readline for full-featured input. Create internal/repl/repl.go with REPL struct and Run() method. Implement command routing to detect special commands vs natural language. Add graceful shutdown on exit/quit commands. Support command history in memory.","acceptance_criteria":"- vc repl command starts interactive shell\n- Readline with history support\n- exit and quit commands work\n- help command shows available commands\n- Ctrl+D exits gracefully\n- Colored prompt and output","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-14T19:08:16.02028-07:00","updated_at":"2025-10-15T11:52:52.233117-07:00","closed_at":"2025-10-14T19:14:34.993548-07:00","dependencies":[{"issue_id":"vc-68","depends_on_id":"vc-9","type":"parent-child","created_at":"2025-10-14T19:08:57.549256-07:00","created_by":"stevey"}]}
{"id":"vc-69","title":"Status display commands (status, ready, blocked)","description":"Implement commands to show project state: ready work, blocked issues, in-progress work. Gives users visibility into tracker state.","design":"Add status.go in internal/repl/ with functions to display tracker state. Use storage.GetReadyWork() for ready command. Query blocked issues. Show in-progress issues. Use color-coded output for clarity. Include issue counts and priorities.","acceptance_criteria":"- status command shows overview (ready/blocked/in-progress counts)\n- ready command lists ready work with priorities\n- blocked command shows blocked issues and their blockers\n- Clean, color-coded output\n- Integration with storage layer","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-14T19:08:39.277899-07:00","updated_at":"2025-10-15T11:52:52.233381-07:00","closed_at":"2025-10-14T19:18:57.8975-07:00","dependencies":[{"issue_id":"vc-69","depends_on_id":"vc-9","type":"parent-child","created_at":"2025-10-14T19:08:57.568104-07:00","created_by":"stevey"},{"issue_id":"vc-69","depends_on_id":"vc-68","type":"blocks","created_at":"2025-10-14T19:08:59.013475-07:00","created_by":"stevey"}]}
{"id":"vc-7","title":"AI Supervision (Assess and Analyze)","description":"Add AI assessment before execution and AI analysis after execution. This is what makes vc intelligent - AI reviews every task and extracts hidden work. The 'secret sauce' that prevents agents from going off the rails.","design":"Integrate Anthropic Go SDK (Sonnet 4.5): 1) assessIssueState before execution (strategy, steps, risks, confidence), 2) analyzeExecutionResult after execution (completion status, punted items, discovered work, quality issues), 3) Auto-create discovered issues from AI analysis, 4) Log AI confidence and reasoning for debugging. Two AI calls per issue execution.","acceptance_criteria":"- Anthropic Go SDK integrated\n- assessIssueState activity implemented\n- analyzeExecutionResult activity implemented\n- AI confidence scores logged\n- AI reasoning/strategy logged to events\n- Auto-creation of discovered issues working\n- Integration with issue processor (Phase 2)\n- AI costs tracked and logged\n- Fallback handling for AI failures","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-13T21:05:19.449508-07:00","updated_at":"2025-10-15T19:48:22.168717-07:00","closed_at":"2025-10-14T14:33:47.206491-07:00","dependencies":[{"issue_id":"vc-7","depends_on_id":"vc-6","type":"blocks","created_at":"2025-10-13T21:05:19.450326-07:00","created_by":"import"}]}
{"id":"vc-70","title":"AI conversation handler with agent spawning","description":"AI service that translates natural language input into structured issue definitions. Core feature enabling natural language interface.","design":"Create translator.go in internal/repl/. Use Anthropic API similar to AI Supervisor. Prompt engineering to extract: title, description, type (bug/feature/epic/task), priority. For complex requests, AI should create epic with child tasks. Handle edge cases (ambiguous input, clarification needed). Return structured issue data for creation.","acceptance_criteria":"- Translate simple requests to issues (e.g. 'Add login page')\n- Detect issue type from context (bug/feature/task/epic)\n- Infer reasonable priority\n- Handle complex requests by creating epics with subtasks\n- Error handling for unclear input\n- Integration tests with mock AI responses","notes":"Updated design: REPL should work like Claude Code. All non-slash-command input goes to Claude API which:\n1. Interprets user intent\n2. Can respond directly for questions\n3. Can create issues if needed\n4. Can spawn worker agents immediately to execute work\n5. Has function calling access to tracker operations\n\nThis is the VibeCoder Primitive - conversational interface with orchestration awareness.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-14T19:08:40.675792-07:00","updated_at":"2025-10-15T11:52:52.234527-07:00","closed_at":"2025-10-14T19:20:33.409557-07:00","dependencies":[{"issue_id":"vc-70","depends_on_id":"vc-9","type":"parent-child","created_at":"2025-10-14T19:08:57.583192-07:00","created_by":"stevey"},{"issue_id":"vc-70","depends_on_id":"vc-68","type":"blocks","created_at":"2025-10-14T19:08:59.032722-07:00","created_by":"stevey"}]}
{"id":"vc-71","title":"Implement 'continue' command to resume execution","description":"The VibeCoder Primitive: 'let's continue' finds ready work and resumes execution. Can start executor or run single issue.","design":"Add continue.go in internal/repl/. Check for ready work. If none, inform user. If available, show options: 1) Run executor in background, 2) Execute single issue interactively, 3) Just show ready work. For single issue execution, show assessment, spawn agent, show real-time output. For background executor, show status updates.","acceptance_criteria":"- 'continue' command finds ready work\n- Shows user what's available\n- Option to execute single issue\n- Option to start background executor\n- Real-time status updates\n- Graceful handling when no work available","status":"open","priority":0,"issue_type":"task","created_at":"2025-10-14T19:08:42.190039-07:00","updated_at":"2025-10-15T11:52:52.2347-07:00","closed_at":"2025-10-14T23:26:12.305744-07:00","dependencies":[{"issue_id":"vc-71","depends_on_id":"vc-9","type":"parent-child","created_at":"2025-10-14T19:08:57.599888-07:00","created_by":"stevey"},{"issue_id":"vc-71","depends_on_id":"vc-68","type":"blocks","created_at":"2025-10-14T19:08:59.047987-07:00","created_by":"stevey"},{"issue_id":"vc-71","depends_on_id":"vc-69","type":"blocks","created_at":"2025-10-14T19:08:59.066391-07:00","created_by":"stevey"}]}
{"id":"vc-72","title":"Polish REPL UX (tab completion, history persistence, Ctrl+C handling)","description":"Final polish: tab completion, persistent history, better error messages, Ctrl+C handling. Makes REPL production-ready.","design":"Add tab completion for commands using readline.PrefixCompleter. Persist history to ~/.vc/repl_history. Intercept Ctrl+C to not exit on first press (require exit command). Add better help text with examples. Improve color scheme for readability. Add welcome banner with quick start guide.","acceptance_criteria":"- Tab completion works for commands\n- Command history persists across sessions\n- Ctrl+C doesn't exit immediately\n- help shows useful examples\n- Welcome banner on startup\n- Clean, professional UX","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-14T19:08:43.670556-07:00","updated_at":"2025-10-15T11:52:52.234893-07:00","dependencies":[{"issue_id":"vc-72","depends_on_id":"vc-9","type":"parent-child","created_at":"2025-10-14T19:08:57.61363-07:00","created_by":"stevey"},{"issue_id":"vc-72","depends_on_id":"vc-68","type":"blocks","created_at":"2025-10-14T19:08:59.08178-07:00","created_by":"stevey"},{"issue_id":"vc-72","depends_on_id":"vc-69","type":"blocks","created_at":"2025-10-14T19:08:59.099022-07:00","created_by":"stevey"},{"issue_id":"vc-72","depends_on_id":"vc-70","type":"blocks","created_at":"2025-10-14T19:08:59.111291-07:00","created_by":"stevey"},{"issue_id":"vc-72","depends_on_id":"vc-71","type":"blocks","created_at":"2025-10-14T19:08:59.123369-07:00","created_by":"stevey"}]}
{"id":"vc-73","title":"Basic Dogfooding MVP - Make VC usable on another project","description":"Minimum viable VC to dogfood on a simple external project. The basic loop: user describes work → AI creates issues → /continue spawns worker → worker executes → results analyzed → follow-on issues created → repeat.","design":"**MVP Loop:**\n1. User in REPL: 'Add Docker support'\n2. AI creates epic + child issues\n3. User: '/continue'\n4. VC finds ready work, spawns Claude Code worker\n5. Worker executes, returns results\n6. AI analyzes, creates follow-on issues\n7. Repeat\n\n**Components needed:**\n- AI conversation → issue creation (function calling)\n- /continue command implementation\n- Worker spawning (Claude Code integration)\n- Results collection and storage\n- Basic activity feed (console output for now)\n- Simple test project (not VC itself)\n\n**Out of scope for MVP:**\n- Workflow automation (code → review → test)\n- Sandbox/worktree management\n- Swarming\n- Cost optimization\n- Full activity feed streaming\n\n**Success criteria:**\nCan fix a real bug or add a real feature to a simple external project using only VC REPL.","acceptance_criteria":"- AI in REPL creates issues from natural language\n- /continue command finds ready work\n- Worker spawns for single issue\n- Worker executes task completely\n- Results captured and stored\n- AI analyzes results and creates follow-ons if needed\n- Can complete a simple bug fix end-to-end\n- Can complete a simple feature addition end-to-end\n- Tested on external project (not VC)","status":"open","priority":0,"issue_type":"epic","created_at":"2025-10-14T19:46:13.808917-07:00","updated_at":"2025-10-15T11:52:52.235196-07:00"}
{"id":"vc-74","title":"Add function calling to AI conversation for issue creation","description":"Enable AI in REPL to create issues directly from conversation. Use Anthropic function calling to give the AI tools: create_issue, create_epic, add_dependency, get_ready_work, get_issue. When user says 'Add Docker support', AI should create appropriate issues automatically.","design":"Extend ConversationHandler to support function calling. Define tools:\n- create_issue(title, description, type, priority, design, acceptance)\n- create_epic(title, description) → returns ID\n- add_child_to_epic(epic_id, child_issue_id, blocks=true)\n- get_ready_work(limit=5)\n- get_issue(issue_id)\n\nUpdate system prompt to explain when to use each tool. AI should proactively create issues when user requests work.","acceptance_criteria":"- AI conversation supports function calling\n- Can create issues from natural language\n- Can create epics with children\n- Can query tracker state\n- Works in REPL: User: 'Add login page' → AI creates issue\n- Works in REPL: User: 'Build auth system' → AI creates epic + children","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-14T19:46:52.124207-07:00","updated_at":"2025-10-15T11:52:52.235475-07:00","closed_at":"2025-10-14T23:11:23.23544-07:00","dependencies":[{"issue_id":"vc-74","depends_on_id":"vc-73","type":"parent-child","created_at":"2025-10-14T19:47:13.943742-07:00","created_by":"stevey"}]}
{"id":"vc-75","title":"Implement worker agent spawning from continue command","description":"The /continue command should find ready work and spawn a Claude Code worker to execute it. Single worker, single task for MVP. Collect stdout/stderr and exit code.","design":"Extend internal/executor/agent.go to support interactive spawning (not just from executor loop). Create spawnWorkerForIssue(ctx, issue) that:\n1. Prepares working directory\n2. Builds prompt with issue context\n3. Spawns Claude Code subprocess\n4. Streams output to console\n5. Waits for completion\n6. Returns result\n\nContinue command uses this to execute single issue interactively from REPL.","acceptance_criteria":"- /continue finds ready work\n- Shows user what will be executed\n- Spawns Claude Code worker\n- Shows worker output in real-time\n- Captures results\n- Updates issue status\n- Returns to REPL when done","status":"open","priority":0,"issue_type":"task","created_at":"2025-10-14T19:46:53.259038-07:00","updated_at":"2025-10-15T11:52:52.235636-07:00","closed_at":"2025-10-14T23:22:14.331605-07:00","dependencies":[{"issue_id":"vc-75","depends_on_id":"vc-73","type":"parent-child","created_at":"2025-10-14T19:47:13.955712-07:00","created_by":"stevey"},{"issue_id":"vc-75","depends_on_id":"vc-74","type":"blocks","created_at":"2025-10-14T19:47:15.622379-07:00","created_by":"stevey"}]}
{"id":"vc-76","title":"Implement results collection and tracker updates","description":"After worker completes, collect results, run AI analysis, update issue status, create follow-on issues. Close loop from execution back to tracker.","design":"In continue command handler:\n1. Worker completes with AgentResult\n2. Extract output and exit code\n3. Call AI supervisor AnalyzeExecutionResult\n4. Parse discovered issues\n5. Create them with CreateDiscoveredIssues\n6. Update parent issue status (close if complete)\n7. Show summary to user\n\nReuse existing AI supervisor code from executor.","acceptance_criteria":"- Worker results captured\n- AI analysis runs automatically\n- Follow-on issues created if discovered\n- Parent issue closed if analysis says complete\n- User sees summary of what happened\n- Tracker state updated correctly","status":"open","priority":0,"issue_type":"task","created_at":"2025-10-14T19:46:54.570274-07:00","updated_at":"2025-10-15T11:52:52.235818-07:00","dependencies":[{"issue_id":"vc-76","depends_on_id":"vc-73","type":"parent-child","created_at":"2025-10-14T19:47:13.966385-07:00","created_by":"stevey"},{"issue_id":"vc-76","depends_on_id":"vc-75","type":"blocks","created_at":"2025-10-14T19:47:15.634062-07:00","created_by":"stevey"}]}
{"id":"vc-77","title":"Add basic activity logging to REPL","description":"Show what's happening during execution. For MVP, just console output with timestamps. Full activity feed (vc-1) comes later.","design":"Add timestamped logging to REPL:\n- Issue claimed\n- Worker spawned\n- Worker output (streamed)\n- Worker completed\n- AI analysis started\n- AI analysis completed\n- Follow-on issues created\n- Issue closed\n\nUse color-coded output. Keep it simple - just fmt.Printf with colors.","acceptance_criteria":"- User sees what's happening\n- Timestamps on major events\n- Color-coded output\n- Worker output visible\n- Clear indication when done","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-14T19:46:55.950832-07:00","updated_at":"2025-10-15T11:52:52.235991-07:00","dependencies":[{"issue_id":"vc-77","depends_on_id":"vc-73","type":"parent-child","created_at":"2025-10-14T19:47:13.976967-07:00","created_by":"stevey"}]}
{"id":"vc-78","title":"Test MVP on external project (simple Go CLI)","description":"Create or choose a simple external Go CLI project. Test the full loop: describe work → issues created → /continue → execution → results → follow-ons. Validate end-to-end.","design":"Choose or create test project:\n- Option 1: Create simple todo CLI\n- Option 2: Use existing small open source Go project\n- Option 3: Simple HTTP server\n\nTest scenarios:\n1. Bug fix: 'Fix the error handling in parseArgs'\n2. Feature: 'Add --verbose flag'\n3. Multi-step: 'Add JSON output support'\n\nDocument what works and what doesn't. File issues for any problems.","acceptance_criteria":"- External project set up\n- Can describe work in REPL\n- Issues created automatically\n- /continue executes work\n- Results analyzed correctly\n- Follow-on issues created if needed\n- At least 2 complete bug fixes\n- At least 1 complete feature addition","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-14T19:46:57.152051-07:00","updated_at":"2025-10-15T11:52:52.236165-07:00","dependencies":[{"issue_id":"vc-78","depends_on_id":"vc-73","type":"parent-child","created_at":"2025-10-14T19:47:13.989102-07:00","created_by":"stevey"},{"issue_id":"vc-78","depends_on_id":"vc-74","type":"blocks","created_at":"2025-10-14T19:47:15.644994-07:00","created_by":"stevey"},{"issue_id":"vc-78","depends_on_id":"vc-75","type":"blocks","created_at":"2025-10-14T19:47:15.656215-07:00","created_by":"stevey"},{"issue_id":"vc-78","depends_on_id":"vc-76","type":"blocks","created_at":"2025-10-14T19:47:15.66597-07:00","created_by":"stevey"},{"issue_id":"vc-78","depends_on_id":"vc-77","type":"blocks","created_at":"2025-10-14T19:47:15.675562-07:00","created_by":"stevey"}]}
{"id":"vc-79","title":"Engineer-in-a-Box Workflow Automation","description":"Automatic workflow filing for the complete inner loop: code → review → test → gates → docs → git. Each worker that completes triggers the next phase automatically. This is Phase 2 after MVP.","design":"**Inner Loop Workflow:**\n\nFor each ready issue:\n1. Worker implements the code\n2. Haiku decides if code review needed (based on diff analysis, NOT heuristics)\n3. If needed: Review worker files granular issues for fixes\n4. High-priority fixes surface in queue via priority + dependencies\n5. Test checker reviews test sufficiency, files issues\n6. Quality gates run (lint, etc.) - worker can fix or defer\n7. Docs/cleanup\n8. Git operations (rebase, etc.)\n\n**Key principles:**\n- Zero Framework Cognition: AI makes decisions, not heuristics\n- Granular issues: Each fix is its own issue\n- No explicit 'implement review fixes' issue - dependencies handle it\n- Recursive review: Code review that touches significant code triggers another review\n\n**Out of scope (Phase 3+):**\n- Sandbox/worktree management\n- Mission planning\n- Swarming","acceptance_criteria":"- Worker completion triggers workflow automatically\n- Haiku-based review decision (not line count heuristic)\n- Review worker files granular fix issues\n- Test checker files test improvement issues\n- Quality gates auto-run\n- Recursive review triggers correctly\n- Full inner loop completes without human intervention\n- Can complete multi-step feature end-to-end","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-14T19:55:43.813305-07:00","updated_at":"2025-10-15T11:52:52.236313-07:00","dependencies":[{"issue_id":"vc-79","depends_on_id":"vc-73","type":"blocks","created_at":"2025-10-14T19:55:47.571038-07:00","created_by":"stevey"}]}
{"id":"vc-8","title":"Quality Gates Enforcement","description":"Enforce quality standards before closing issues. All four gates must pass: go test, golangci-lint, go build. On failure, create blocking issues. Prevents broken code from being marked complete.","design":"After issue execution completes, run quality gate sequence: 1) go test (all tests must pass), 2) golangci-lint (zero lint errors), 3) go build (clean build). On any gate failure, create blocking issue with gate type label, mark original issue as blocked. Quality gates are mechanical - no AI decision making.","acceptance_criteria":"- go test gate implemented\n- golangci-lint gate implemented  \n- go build gate implemented\n- Blocking issue creation on failures\n- Gate failure details captured in blocking issues\n- Original issue status updated to blocked\n- Gate pass/fail logged to events\n- Integration with issue processor\n- Gate results visible in issue show command","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-13T21:05:19.449594-07:00","updated_at":"2025-10-15T19:48:22.111351-07:00","closed_at":"2025-10-14T16:08:24.052072-07:00","dependencies":[{"issue_id":"vc-8","depends_on_id":"vc-7","type":"blocks","created_at":"2025-10-13T21:05:19.450421-07:00","created_by":"import"}]}
{"id":"vc-80","title":"Haiku-based code review trigger (ZFC principle)","description":"After any implementation, use Haiku to decide if code review is warranted. NO heuristics like line counts or file counts. AI understands semantic significance.","design":"**ZFC Violation to Fix:**\nOld way: 'If \u003e50 lines or \u003e10 files, trigger review' ← arbitrary heuristic\nNew way: Let Haiku decide based on actual diff analysis\n\n**Implementation:**\nAfter worker completes any issue:\n1. Get git diff of changes\n2. Send to Haiku (cheap, fast) with prompt:\n   'Analyze this diff. Does it warrant a code review?\n    Consider:\n    - Complexity and risk\n    - Critical paths touched (auth, security, data integrity)\n    - Test coverage\n    - Refactoring vs new features\n    - API changes\n    Return JSON: { needs_review: bool, reasoning: string }'\n3. If needs_review=true: File code review issue\n4. Log reasoning in comment\n\n**Cost:** ~$0.001 per check (Haiku)\n**Benefit:** Smart decisions vs arbitrary thresholds\n\n**Examples where heuristics fail:**\n- 10 line security change → SHOULD review\n- 200 line generated test boilerplate → probably NOT\n- 30 line auth refactor → SHOULD review\n- 100 line dependency update → maybe NOT\n\nHaiku understands context, heuristics don't.","acceptance_criteria":"- Haiku analyzes all completed work diffs\n- Decision based on semantic analysis not line counts\n- Reasoning logged for transparency\n- False positive rate \u003c 10% (unnecessary reviews)\n- False negative rate \u003c 5% (missed needed reviews)\n- Cost per decision \u003c $0.002\n- Integration with workflow automation","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-14T19:55:45.228203-07:00","updated_at":"2025-10-15T11:52:52.236695-07:00","dependencies":[{"issue_id":"vc-80","depends_on_id":"vc-79","type":"parent-child","created_at":"2025-10-14T19:55:46.365522-07:00","created_by":"stevey"}]}
{"id":"vc-81","title":"Add tests for conversation handler tool functions","description":"Add comprehensive tests for the AI conversation handler's tool execution functions. Cover valid inputs, invalid inputs, error handling, and integration with storage layer.","design":"Create internal/repl/conversation_test.go with tests for:\n- toolCreateIssue: valid/invalid types, missing fields, storage errors\n- toolCreateEpic: valid creation, error cases\n- toolAddChildToEpic: parent-child relationship, blocking relationships, both together\n- toolGetReadyWork: different limits, empty results\n- toolGetIssue: valid issue, missing issue\n- executeTool: dispatching to correct handler, error handling\n- Conversation loop: max iterations limit, tool use flow\n\nUse mock storage for unit tests. Add integration tests with real SQLite database.","acceptance_criteria":"- Tests for all tool handler functions\n- Valid and invalid input cases covered\n- Error handling tested\n- Mock storage used for unit tests\n- Integration tests with real database\n- All tests pass\n- Code coverage \u003e80% for conversation.go","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-14T21:28:54.503119-07:00","updated_at":"2025-10-15T11:52:52.236883-07:00"}
{"id":"vc-82","title":"Add more conversation tools (update_issue, close_issue, add_dependency)","description":"Extend the AI conversation handler with additional tools for managing issues beyond creation. Allow AI to update issue status/priority, close issues, and add generic dependencies.","design":"Add new tool definitions in getTools():\n- update_issue(issue_id, status?, priority?, notes?): Update issue fields\n- close_issue(issue_id, reason?): Close an issue\n- add_dependency(from_id, to_id, type='blocks'): Generic dependency creation\n- list_issues(status?, type?, limit=10): List issues with filters\n\nImplement corresponding tool handler functions following the pattern of existing handlers. Use AIActor constant for all operations.","acceptance_criteria":"- update_issue tool implemented and working\n- close_issue tool implemented and working\n- add_dependency tool implemented and working\n- list_issues tool implemented and working\n- All tools use AIActor constant\n- Tools added to system prompt documentation\n- Manual testing shows AI can use all tools correctly","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-14T21:29:05.119528-07:00","updated_at":"2025-10-15T11:52:52.237051-07:00"}
{"id":"vc-83","title":"Add rate limiting and safety controls to conversation tool use","description":"Prevent AI from creating unlimited issues or overwhelming the system during conversation. Add safety limits and user warnings.","design":"Add conversation-level tracking:\n- Track tool calls per conversation (reset on ClearHistory)\n- Max issues created per conversation (e.g., 20)\n- Max tool calls per message (e.g., 15)\n- Warn user if AI creates \u003e5 issues in single conversation\n\nConsider adding confirmation for bulk operations:\n- If AI tries to create \u003eN issues, ask user to confirm\n- Add /limits command to show current usage\n- Add ConversationStats struct to track metrics\n\nImplementation:\n- Add counters to ConversationHandler\n- Check limits in executeTool before running\n- Return error if limit exceeded\n- Log warnings for user visibility","acceptance_criteria":"- Max issues per conversation limit enforced\n- Max tool calls per message limit enforced\n- User warned when AI creates many issues\n- Limits are documented and tunable\n- Error messages explain what limit was hit\n- Manual testing shows limits work correctly","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T21:29:16.381618-07:00","updated_at":"2025-10-15T11:52:52.237222-07:00"}
{"id":"vc-84","title":"Return structured JSON tool results instead of formatted strings","description":"Current tool handlers return formatted strings (e.g., 'Created task vc-82: Add feature'). Return structured JSON instead so AI can parse and reference results more reliably.","design":"Change tool handler return values from strings to JSON:\n\nBefore:\nreturn 'Created task vc-82: Add feature', nil\n\nAfter:\nresult := map[string]interface{}{\n    'status': 'success',\n    'action': 'created',\n    'issue_type': 'task',\n    'issue_id': 'vc-82',\n    'title': 'Add feature',\n}\nreturn json.Marshal(result)\n\nBenefits:\n- AI can extract issue IDs reliably\n- Easier to reference created issues in follow-up tools\n- More parseable for automated workflows\n- Still human-readable in conversation\n\nUpdate all tool handlers to return JSON. Keep error returns as strings (they're already structured by the SDK).","acceptance_criteria":"- All tool handlers return JSON objects\n- JSON includes action, status, and relevant IDs\n- Error handling unchanged (returns string errors)\n- AI can parse and use returned issue IDs\n- Manual testing shows AI correctly references created issues\n- Conversation readability not degraded","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T21:29:29.304979-07:00","updated_at":"2025-10-15T11:52:52.237378-07:00"}
{"id":"vc-85","title":"Add godoc comments to conversation handler functions","description":"Add comprehensive godoc comments to all exported and internal functions in conversation.go. Follow Go documentation standards.","design":"Add documentation for:\n- getTools(): Explain tool definitions and function calling\n- executeTool(): Document dispatch pattern and error handling\n- All tool handler functions (toolCreateIssue, etc.): Document parameters, behavior, return format\n\nFollow godoc conventions:\n- Start with function name\n- Describe what it does\n- Document parameters and return values\n- Include examples where helpful\n\nExample:\n// toolCreateIssue creates a new issue from AI conversation.\n// Validates issue type and uses AIActor as creator.\n// Returns formatted success message with issue ID or error.\nfunc (c *ConversationHandler) toolCreateIssue(...) (string, error)","acceptance_criteria":"- All public functions documented\n- All tool handler functions documented\n- Comments follow godoc conventions\n- Comments are accurate and helpful\n- go doc output is readable","status":"open","priority":3,"issue_type":"chore","created_at":"2025-10-14T21:29:40.355746-07:00","updated_at":"2025-10-15T11:52:52.237537-07:00"}
{"id":"vc-86","title":"Improve get_issue tool output format","description":"The get_issue tool currently returns full JSON marshaled Issue struct, which is verbose and hard to read in conversation. Return a concise summary format instead.","design":"Change toolGetIssue from:\njson.MarshalIndent(issue, '', '  ')\n\nTo formatted summary:\nfmt.Sprintf(\"%s [%s] %s\\nStatus: %s | Priority: P%d\\nDescription: %s\\nDependencies: %d blockers, %d children\\nCreated: %s\",\n  issue.ID, issue.IssueType, issue.Title,\n  issue.Status, issue.Priority,\n  truncate(issue.Description, 200),\n  len(blockers), len(children),\n  issue.CreatedAt.Format('2006-01-02'))\n\nInclude only essential fields. Keep it readable in conversation context.","acceptance_criteria":"- get_issue returns concise summary format\n- Includes ID, type, title, status, priority, description (truncated)\n- Shows dependency counts (blockers, children, related)\n- Format is readable in AI conversation\n- Still includes enough detail for AI to work with","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-14T21:29:54.164422-07:00","updated_at":"2025-10-15T11:52:52.237681-07:00"}
{"id":"vc-87","title":"Implement circuit breaker pattern for AI API calls","description":"Add circuit breaker pattern to prevent cascading failures when AI API is unhealthy. Circuit breaker tracks failure rates across requests and temporarily stops making calls when failure threshold is exceeded, giving the service time to recover.\n\nThis completes the resilience strategy from vc-59. Current implementation has timeout + retry, but no circuit breaker.\n\n**Problem:** Without circuit breaker, if AI API is down, every request will:\n- Wait 60s timeout\n- Retry 3 times with backoff (1s, 2s, 4s)\n- Total: ~3 minutes per request before giving up\n- All concurrent requests pile up, consuming resources\n\n**Solution:** Circuit breaker pattern with three states:\n- CLOSED: Normal operation, requests pass through\n- OPEN: Too many failures, block requests immediately (fail fast)\n- HALF_OPEN: Testing if service recovered, allow one request through\n\n**Reference:** Vibecoder used Temporal's built-in circuit breaker. We need lightweight Go implementation.","design":"In internal/ai/supervisor.go:\n\n1. Add CircuitBreaker struct:\n   - State: closed/open/half-open\n   - FailureCount, SuccessCount\n   - LastFailureTime\n   - Thresholds (configurable)\n\n2. Add to RetryConfig:\n   - CircuitBreakerEnabled bool\n   - FailureThreshold int (default: 5)\n   - SuccessThreshold int (default: 2)\n   - OpenTimeout time.Duration (default: 30s)\n\n3. Wrap retryWithBackoff with circuit breaker:\n   - Check state before attempting request\n   - If OPEN, fail immediately with ErrCircuitOpen\n   - If HALF_OPEN, allow single probe request\n   - Update state based on success/failure\n   - Thread-safe with mutex\n\n4. Add metrics logging:\n   - Circuit state transitions\n   - Failure/success counts\n\n**Implementation notes:**\n- Use sync.Mutex for thread safety\n- Log state transitions prominently\n- Make thresholds tunable via config\n- Consider adding metrics export","acceptance_criteria":"- Circuit breaker prevents cascading failures during API outages\n- OPEN state fails fast without retrying\n- HALF_OPEN state probes for recovery\n- State transitions logged with metrics\n- Configurable thresholds (failure/success counts, open timeout)\n- Thread-safe for concurrent requests\n- Tests cover all three states and transitions","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-14T23:30:15.618346-07:00","updated_at":"2025-10-15T11:52:52.237858-07:00","dependencies":[{"issue_id":"vc-87","depends_on_id":"vc-59","type":"discovered-from","created_at":"2025-10-15T00:11:15.374106-07:00","created_by":"auto-import"}]}
{"id":"vc-88","title":"Epic completion logic uses wrong dependency direction","description":"The checkEpicCompletion() function in internal/executor/epic.go:24 uses GetDependencies() to find parent epics, but this is backwards. An epic DEPENDS ON its children, so we need GetDependents() to find which epics depend on the completed issue.\n\nLocation: internal/executor/epic.go line 24\n\nCurrent (incorrect):\ndeps, err := store.GetDependencies(ctx, childIssueID)\n\nShould be:\ndependents, err := store.GetDependents(ctx, childIssueID)\n\nResult: Epic auto-completion won't work because it's looking at the wrong side of the dependency relationship.","acceptance_criteria":"- checkEpicCompletion uses GetDependents() instead of GetDependencies()\n- Parent epics are correctly identified when child completes\n- Epic is only closed when all children are complete\n- Test verifies epic completion with multiple children","notes":"Root cause analysis: The database has TWO different dependency models for epic-child relationships:\n\nOLD MODEL (vc-5, created Oct 13):\n- (parent, child) direction: vc-5 -\u003e vc-10\n- Epic depends on child for completion\n- Code: GetDependencies(epic) returns children\n\nNEW MODEL (vc-73, created Oct 14):  \n- (child, parent) direction: vc-74 -\u003e vc-73\n- Child belongs to parent (standard model)\n- Code: GetDependents(epic) returns children\n\nDECISION: Standardize on (child, parent) because:\n1. Intuitive: \"child belongs to parent\"\n2. Industry standard (Jira, Linear, GitHub)\n3. Clear code: GetDependencies(child) -\u003e parent\n4. Agent-friendly natural queries\n\nACTION NEEDED:\n1. Migrate vc-5 and all Oct 13 epics to use (child, parent)\n2. Keep existing code (it's correct for standard model)\n3. Add tests for standard model\n4. Document the convention","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-10-14T23:30:17.11713-07:00","updated_at":"2025-10-15T11:52:52.238124-07:00","closed_at":"2025-10-15T00:41:46.130348-07:00"}
{"id":"vc-89","title":"Quality gates error handling allows issues to close when gates didn't run","description":"In ResultsProcessor.ProcessAgentResult() at internal/executor/results.go:122-136, if gate runner creation fails, execution continues with GatesPassed=true. This allows issues to be marked complete even though gates never ran.\n\nLocation: internal/executor/results.go lines 122-136\n\nCurrent code:\nif err != nil {\n    fmt.Fprintf(os.Stderr, \"Warning: failed to create quality gate runner: %v (skipping gates)\\n\", err)\n} else {\n    gateResults, allPassed := gateRunner.RunAll(ctx)\n    result.GatesPassed = allPassed\n    ...\n}\n\nProblem: If gateRunner creation fails, result.GatesPassed remains true (default), allowing issue to complete without gates.\n\nFix: Set result.GatesPassed = false and block the issue when gate runner creation fails.","acceptance_criteria":"- Quality gate runner creation failure sets GatesPassed=false\n- Issue is marked as blocked when gates can't run\n- Error comment explains why gates failed\n- Test verifies behavior when gate runner creation fails","status":"open","priority":1,"issue_type":"bug","created_at":"2025-10-14T23:30:18.500753-07:00","updated_at":"2025-10-15T11:52:52.238281-07:00"}
{"id":"vc-9","title":"REPL Shell and Natural Language Interface","description":"Interactive shell for directing VC. Natural language interface that translates user requests into issues. The 'VibeCoder Primitive': user says 'let's continue' and system resumes from tracker state.","design":"Simple vc command with chat interface: 1) Accept natural language input from user, 2) Use AI to translate requests into issues (create epic, break into children), 3) Show activity feed of agent work, 4) 'let's continue' command resumes from current tracker state (finds ready work). The human\u003c-\u003eAI interaction layer.","acceptance_criteria":"- vc repl command starts interactive shell\n- Natural language input accepted\n- AI translation of requests to issues working\n- Issue/epic creation from natural language\n- Activity feed display in REPL\n- 'let's continue' command implemented\n- Tracker state visibility (what's ready, what's blocked)\n- Exit/quit commands\n- Command history\n- Integration with issue processor and AI supervision","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-13T21:05:19.449673-07:00","updated_at":"2025-10-15T19:48:22.083986-07:00","dependencies":[{"issue_id":"vc-9","depends_on_id":"vc-8","type":"blocks","created_at":"2025-10-13T21:05:19.450524-07:00","created_by":"import"}]}
{"id":"vc-90","title":"Migrate epic-child dependencies to standard (child, parent) direction","description":"Database has inconsistent epic-child dependency models:\n\nOLD MODEL (Oct 13 - vc-5, vc-6, vc-7, vc-8, vc-9):\n- Direction: (epic, child) = vc-5 -\u003e vc-10\n- Semantics: Epic depends on child for completion\n- Query: GetDependencies(epic) returns children\n\nNEW MODEL (Oct 14+ - vc-73 onwards):\n- Direction: (child, epic) = vc-74 -\u003e vc-73  \n- Semantics: Child belongs to parent (standard)\n- Query: GetDependents(epic) returns children\n\nSTANDARD: (child, parent) is industry standard (Jira, Linear, GitHub) and more intuitive.\n\nMigration needed:\n1. Find all (parent, child) parent-child dependencies\n2. Reverse to (child, parent)\n3. Verify epic completion logic works\n4. Add tests for standard model","design":"Query to find old-style dependencies:\nSELECT * FROM dependencies \nWHERE type = 'parent-child'\n  AND issue_id IN (SELECT id FROM issues WHERE issue_type = 'epic')\n\nMigration script:\nFOR EACH (epic_id, child_id, 'parent-child'):\n  1. DELETE (epic_id, child_id)\n  2. INSERT (child_id, epic_id, 'parent-child')\n\nVerify by checking vc-5 children appear correctly.","acceptance_criteria":"- All epic-child dependencies use (child, parent) direction\n- GetDependents(epic) returns all children\n- GetDependencies(child) returns parent epic\n- Epic completion logic works for migrated epics\n- Added regression test for standard model","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T00:41:39.126208-07:00","updated_at":"2025-10-15T11:52:52.24005-07:00","closed_at":"2025-10-15T00:51:15.075369-07:00"}
{"id":"vc-91","title":"Sandbox Management System","description":"Implement isolated sandbox creation, management, and teardown for agent execution. Each mission gets its own git worktree/clone with separate branch and beads database.","design":"\n# Architecture\n\n## Core Types\n- Sandbox: Represents isolated work environment\n- SandboxManager: Creates/destroys/hands off sandboxes\n- SandboxConfig: Configuration for sandbox creation\n\n## Key Features\n1. Git worktree creation with dedicated branch\n2. Separate beads database per sandbox\n3. Sandbox lifecycle management (create, use, teardown)\n4. Context preservation for agent handoff\n5. Cleanup on success/failure\n\n## Directory Structure\n```\n.vc/\n  sandboxes/\n    mission-{id}/\n      .git (worktree)\n      .beads/\n        mission.db\n      code/\n```\n\n## Integration Points\n- Executor creates sandbox before spawning agent\n- Agent config includes sandbox path\n- Results processor can access sandbox state\n- Cleanup happens in defer or explicit call\n","acceptance_criteria":"\n- Can create git worktree for a mission\n- Worktree is on dedicated branch (mission-{id})\n- Each sandbox has isolated beads database\n- Can hand off sandbox context between agents\n- Can inspect sandbox state (git status, modified files)\n- Automatic cleanup on completion/failure\n- No interference between concurrent sandboxes\n- Integration tests with real git repos\n","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-15T01:22:28.237903-07:00","updated_at":"2025-10-15T19:47:29.774384-07:00"}
{"id":"vc-92","title":"Design sandbox package types and interfaces","description":"Define core types, interfaces, and configuration for sandbox management","design":"\nCreate internal/sandbox/types.go with:\n\n## Types\n```go\ntype Sandbox struct {\n    ID          string    // Unique sandbox ID\n    MissionID   string    // Associated mission/epic ID\n    Path        string    // Absolute path to sandbox root\n    GitBranch   string    // Dedicated git branch\n    GitWorktree string    // Path to git worktree\n    BeadsDB     string    // Path to sandbox-local beads DB\n    ParentRepo  string    // Original repo path\n    Created     time.Time\n    LastUsed    time.Time\n    Status      SandboxStatus\n}\n\ntype SandboxStatus string\nconst (\n    SandboxStatusActive    SandboxStatus = \"active\"\n    SandboxStatusCompleted SandboxStatus = \"completed\"\n    SandboxStatusFailed    SandboxStatus = \"failed\"\n    SandboxStatusCleaned   SandboxStatus = \"cleaned\"\n)\n\ntype SandboxConfig struct {\n    MissionID     string\n    ParentRepo    string\n    BaseBranch    string // Branch to create worktree from\n    SandboxRoot   string // Where to create sandboxes\n    PreserveOnFailure bool\n}\n\ntype SandboxContext struct {\n    Sandbox      *Sandbox\n    GitStatus    string\n    ModifiedFiles []string\n    LastCommand   string\n    WorkState     map[string]interface{}\n}\n```\n\n## Interface\n```go\ntype Manager interface {\n    Create(ctx context.Context, cfg SandboxConfig) (*Sandbox, error)\n    Get(ctx context.Context, id string) (*Sandbox, error)\n    List(ctx context.Context) ([]*Sandbox, error)\n    InspectState(ctx context.Context, sandbox *Sandbox) (*SandboxContext, error)\n    Cleanup(ctx context.Context, sandbox *Sandbox) error\n    CleanupAll(ctx context.Context, olderThan time.Duration) error\n}\n```\n","acceptance_criteria":"\n- Sandbox type with all required fields\n- SandboxStatus enum defined\n- SandboxConfig for creation parameters\n- SandboxContext for state handoff\n- Manager interface defined\n- All types exported and documented\n- No external dependencies yet (just types)\n","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:22:43.797247-07:00","updated_at":"2025-10-15T19:47:27.900192-07:00","dependencies":[{"issue_id":"vc-92","depends_on_id":"vc-91","type":"parent-child","created_at":"2025-10-15T01:22:43.799351-07:00","created_by":"stevey"}]}
{"id":"vc-93","title":"Implement git worktree creation and management","description":"Implement git worktree operations for sandbox isolation","design":"\nCreate internal/sandbox/git.go with git worktree operations:\n\n## Functions\n```go\n// createWorktree creates a git worktree for the sandbox\nfunc createWorktree(ctx context.Context, cfg SandboxConfig, branchName string) (string, error)\n\n// removeWorktree removes a git worktree\nfunc removeWorktree(ctx context.Context, worktreePath string) error\n\n// getGitStatus returns current git status in worktree\nfunc getGitStatus(ctx context.Context, worktreePath string) (string, error)\n\n// getModifiedFiles returns list of modified files\nfunc getModifiedFiles(ctx context.Context, worktreePath string) ([]string, error)\n\n// createBranch creates a new branch in the worktree\nfunc createBranch(ctx context.Context, worktreePath, branchName, baseBranch string) error\n```\n\n## Implementation Notes\n- Use exec.Command to call git\n- Handle git worktree add with --detach\n- Create branch after worktree creation\n- Validate git repo before operations\n- Return detailed errors for debugging\n- Support both absolute and relative paths\n- Clean up on errors (defer removal)\n\n## Error Handling\n- Detect if not a git repo\n- Handle branch already exists\n- Handle worktree path conflicts\n- Validate parent repo state\n","acceptance_criteria":"\n- Can create git worktree from parent repo\n- Worktree is on dedicated branch (mission-{id})\n- Can get git status from worktree\n- Can list modified files\n- Can remove worktree cleanly\n- Handles errors gracefully\n- Works with detached HEAD state\n- Unit tests with temp git repos\n","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:22:57.215662-07:00","updated_at":"2025-10-15T19:48:13.08978-07:00","dependencies":[{"issue_id":"vc-93","depends_on_id":"vc-91","type":"parent-child","created_at":"2025-10-15T01:22:57.216105-07:00","created_by":"stevey"},{"issue_id":"vc-93","depends_on_id":"vc-92","type":"blocks","created_at":"2025-10-15T01:22:57.216343-07:00","created_by":"stevey"}]}
{"id":"vc-94","title":"Implement sandbox database initialization","description":"Initialize isolated beads database for each sandbox","design":"\nCreate internal/sandbox/database.go for sandbox DB management:\n\n## Functions\n```go\n// initSandboxDB creates and initializes a beads database for the sandbox\nfunc initSandboxDB(ctx context.Context, sandboxPath, missionID string) (string, error)\n\n// copyCoreIssues copies mission and its dependencies to sandbox DB\nfunc copyCoreIssues(ctx context.Context, mainDB, sandboxDB storage.Storage, missionID string) error\n\n// mergeResults merges completed work from sandbox DB back to main DB\nfunc mergeResults(ctx context.Context, sandboxDB, mainDB storage.Storage, missionID string) error\n```\n\n## Implementation\n1. Create .beads/ directory in sandbox\n2. Initialize SQLite database\n3. Copy mission issue and all blocking dependencies\n4. Copy child issues of the mission\n5. Mark sandbox metadata (parent DB, mission ID)\n6. On completion, merge discovered issues and status updates\n\n## Metadata to Track\n- parent_db_path: Path to main database\n- mission_id: Root mission this sandbox serves\n- created_at: Sandbox creation time\n- sandbox_id: Unique identifier\n","acceptance_criteria":"\n- Creates .beads/mission.db in sandbox\n- Database is properly initialized with schema\n- Mission issue copied to sandbox DB\n- Dependencies copied recursively\n- Child issues copied\n- Metadata tracks sandbox provenance\n- Can merge results back to main DB\n- Unit tests with temp databases\n","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:23:10.094761-07:00","updated_at":"2025-10-15T19:48:13.065647-07:00","dependencies":[{"issue_id":"vc-94","depends_on_id":"vc-91","type":"parent-child","created_at":"2025-10-15T01:23:10.096474-07:00","created_by":"stevey"},{"issue_id":"vc-94","depends_on_id":"vc-92","type":"blocks","created_at":"2025-10-15T01:23:10.097027-07:00","created_by":"stevey"}]}
{"id":"vc-95","title":"Implement SandboxManager with create/cleanup operations","description":"Implement the main SandboxManager that orchestrates sandbox lifecycle","design":"\nCreate internal/sandbox/manager.go:\n\n## Manager struct\n```go\ntype manager struct {\n    config       Config\n    activeSandboxes map[string]*Sandbox\n    mu           sync.RWMutex\n    store        storage.Storage // Main database\n}\n\ntype Config struct {\n    SandboxRoot       string\n    ParentRepo        string\n    MainDB            storage.Storage\n    PreserveOnFailure bool\n    MaxAge            time.Duration\n}\n```\n\n## Key Methods\n```go\nfunc NewManager(cfg Config) (Manager, error)\n\nfunc (m *manager) Create(ctx context.Context, cfg SandboxConfig) (*Sandbox, error) {\n    // 1. Generate unique sandbox ID\n    // 2. Create sandbox directory structure\n    // 3. Create git worktree with dedicated branch\n    // 4. Initialize beads database\n    // 5. Copy mission and dependencies to sandbox DB\n    // 6. Register sandbox in tracking map\n    // 7. Return Sandbox handle\n}\n\nfunc (m *manager) Cleanup(ctx context.Context, sandbox *Sandbox) error {\n    // 1. Merge results if needed\n    // 2. Remove git worktree\n    // 3. Remove sandbox directory (unless PreserveOnFailure)\n    // 4. Update sandbox status\n    // 5. Remove from active map\n}\n\nfunc (m *manager) InspectState(ctx context.Context, sandbox *Sandbox) (*SandboxContext, error) {\n    // 1. Get git status\n    // 2. Get modified files\n    // 3. Read sandbox metadata\n    // 4. Return context for agent briefing\n}\n```\n\n## Directory Structure\n```\n{SandboxRoot}/\n  mission-{id}-{timestamp}/\n    .git -\u003e worktree\n    .beads/\n      mission.db\n      metadata.json\n    code/\n```\n","acceptance_criteria":"\n- NewManager creates manager with config\n- Create() generates isolated sandbox\n- Sandbox has git worktree on dedicated branch\n- Sandbox has initialized beads database\n- InspectState() returns current sandbox state\n- Cleanup() removes worktree and directory\n- Cleanup() merges results to main DB\n- PreserveOnFailure flag works correctly\n- Thread-safe for concurrent operations\n- Integration tests with real git repos and databases\n","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:23:25.483049-07:00","updated_at":"2025-10-15T19:48:13.04006-07:00","dependencies":[{"issue_id":"vc-95","depends_on_id":"vc-91","type":"parent-child","created_at":"2025-10-15T01:23:25.485023-07:00","created_by":"stevey"},{"issue_id":"vc-95","depends_on_id":"vc-93","type":"blocks","created_at":"2025-10-15T01:23:25.48574-07:00","created_by":"stevey"},{"issue_id":"vc-95","depends_on_id":"vc-94","type":"blocks","created_at":"2025-10-15T01:23:25.486016-07:00","created_by":"stevey"}]}
{"id":"vc-96","title":"Integrate sandbox management into executor","description":"Modify executor to create sandboxes before spawning agents and cleanup after","design":"\nModify internal/executor/executor.go:\n\n## Changes to Executor\n```go\ntype Executor struct {\n    // ... existing fields ...\n    sandboxMgr sandbox.Manager\n}\n\ntype Config struct {\n    // ... existing fields ...\n    SandboxRoot    string\n    ParentRepo     string\n    EnableSandboxes bool // Feature flag\n}\n```\n\n## Modified executeIssue() Flow\n```go\nfunc (e *Executor) executeIssue(ctx context.Context, issue *types.Issue) error {\n    // 1. AI Assessment (existing)\n    \n    // 2. Create sandbox if enabled\n    var sandbox *sandbox.Sandbox\n    if e.config.EnableSandboxes {\n        sandbox, err = e.sandboxMgr.Create(ctx, sandbox.SandboxConfig{\n            MissionID:  issue.ID,\n            ParentRepo: e.config.ParentRepo,\n            BaseBranch: \"main\",\n        })\n        if err != nil {\n            return err\n        }\n        defer e.sandboxMgr.Cleanup(ctx, sandbox)\n    }\n    \n    // 3. Spawn agent with sandbox path\n    agentCfg := AgentConfig{\n        // ... existing fields ...\n        WorkingDir: sandbox.Path, // Use sandbox instead of \".\"\n        Sandbox:    sandbox,       // Pass sandbox context\n    }\n    \n    // 4. Execute (existing)\n    // 5. Process results (existing)\n    // 6. Cleanup handled by defer\n}\n```\n\n## Configuration\n- Add --sandbox-root flag to execute command\n- Add --enable-sandboxes flag (default: false for now)\n- Auto-detect parent repo from current directory\n","acceptance_criteria":"\n- Executor has sandboxMgr field\n- executeIssue() creates sandbox before spawning agent\n- Agent WorkingDir points to sandbox path\n- Sandbox is cleaned up via defer\n- Feature flag allows disabling sandboxes\n- Configuration flags added to execute command\n- Integration test: executor creates sandbox, runs agent, cleans up\n- Works with both sandbox enabled and disabled\n","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:23:40.18224-07:00","updated_at":"2025-10-15T19:48:13.021655-07:00","dependencies":[{"issue_id":"vc-96","depends_on_id":"vc-91","type":"parent-child","created_at":"2025-10-15T01:23:40.190824-07:00","created_by":"stevey"},{"issue_id":"vc-96","depends_on_id":"vc-95","type":"blocks","created_at":"2025-10-15T01:23:40.191416-07:00","created_by":"stevey"}]}
{"id":"vc-97","title":"Enhanced Context Management and Prompting","description":"Enhance agent prompting with rich context including sandbox location, mission hierarchy, previous attempts, related issues, and quality gate failures. Enable nondeterministic idempotence through comprehensive state briefing.","design":"\n# Architecture\n\n## Core Components\n1. PromptBuilder: Assembles context from multiple sources\n2. ContextGatherer: Collects relevant context data\n3. PromptTemplate: Structured prompt formatting\n\n## Context Sources\n- Issue details (title, description, design, acceptance)\n- Sandbox location and setup instructions\n- Parent mission context (for child tasks)\n- Related issues (blockers, dependents, siblings)\n- Previous execution attempts and their output\n- Quality gate failures and their details\n- Code review feedback (for fix tasks)\n- Git state (current branch, uncommitted changes)\n- Beads database state (ready work, blocked issues)\n\n## Prompt Structure\n```markdown\n# Mission Context\n{Parent mission details if this is a child task}\n\n# Your Task\n{Issue title and description}\n\n# Environment\n- Sandbox: {path}\n- Branch: {branch}\n- Database: {db path}\n\n# Design Notes\n{Design field}\n\n# Acceptance Criteria\n{Criteria}\n\n# Related Work\n{Blocking issues, related issues}\n\n# Previous Attempts\n{Summary of previous runs if any}\n\n# Current State\n{Git status, modified files, where we left off}\n```\n\n## Nondeterministic Idempotence\nPrompt includes \"where we left off\" analysis:\n- What was attempted\n- What succeeded\n- What failed\n- What remains to be done\n- Current sandbox state\n","acceptance_criteria":"\n- PromptBuilder assembles context from multiple sources\n- Includes sandbox environment details\n- Includes parent mission context for child tasks\n- Includes previous attempt history\n- Includes quality gate failures\n- Includes git state analysis\n- Supports 'resume from interruption' scenarios\n- Prompt is comprehensive but readable\n- Integration tests verify all context sources\n- Works with and without sandbox\n","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-15T01:23:59.994645-07:00","updated_at":"2025-10-15T19:47:19.943765-07:00"}
{"id":"vc-98","title":"Design PromptContext types and ContextGatherer interface","description":"Define types for comprehensive context gathering and prompt building","design":"\nCreate internal/executor/context.go:\n\n## Types\n```go\ntype PromptContext struct {\n    Issue             *types.Issue\n    Sandbox           *sandbox.SandboxContext\n    ParentMission     *types.Issue\n    RelatedIssues     *RelatedIssues\n    PreviousAttempts  []*ExecutionAttempt\n    QualityGateStatus *gates.GateStatus\n    GitState          *GitState\n    ResumeHint        string // \"where we left off\" summary\n}\n\ntype RelatedIssues struct {\n    Blockers  []*types.Issue // Issues blocking this one\n    Dependents []*types.Issue // Issues depending on this one\n    Siblings   []*types.Issue // Other children of same parent\n    Related    []*types.Issue // Related but not blocking\n}\n\ntype ExecutionAttempt struct {\n    AttemptNumber int\n    StartedAt     time.Time\n    CompletedAt   time.Time\n    Success       bool\n    Summary       string\n    Output        string // Truncated output\n    Errors        string // Truncated errors\n}\n\ntype GitState struct {\n    CurrentBranch   string\n    UncommittedChanges bool\n    ModifiedFiles   []string\n    Status          string\n}\n\ntype ContextGatherer interface {\n    GatherContext(ctx context.Context, issue *types.Issue, sandbox *sandbox.Sandbox) (*PromptContext, error)\n    GetParentMission(ctx context.Context, issue *types.Issue) (*types.Issue, error)\n    GetRelatedIssues(ctx context.Context, issue *types.Issue) (*RelatedIssues, error)\n    GetPreviousAttempts(ctx context.Context, issueID string) ([]*ExecutionAttempt, error)\n    AnalyzeResumeState(ctx context.Context, sandbox *sandbox.Sandbox, attempts []*ExecutionAttempt) (string, error)\n}\n```\n","acceptance_criteria":"\n- PromptContext type with all required fields\n- RelatedIssues struct for dependency context\n- ExecutionAttempt tracks previous runs\n- GitState captures git status\n- ContextGatherer interface defined\n- Types support both sandboxed and non-sandboxed execution\n- All types exported and documented\n","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:24:15.071713-07:00","updated_at":"2025-10-15T19:47:18.410413-07:00","dependencies":[{"issue_id":"vc-98","depends_on_id":"vc-97","type":"parent-child","created_at":"2025-10-15T01:24:15.073479-07:00","created_by":"stevey"}]}
{"id":"vc-99","title":"Implement execution attempt history tracking","description":"Store and retrieve previous execution attempts for an issue to support resume/retry scenarios","design":"\n## Database Schema Addition\nAdd to storage layer:\n\n```sql\nCREATE TABLE execution_history (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    issue_id TEXT NOT NULL,\n    executor_instance_id TEXT NOT NULL,\n    attempt_number INTEGER NOT NULL,\n    started_at TIMESTAMP NOT NULL,\n    completed_at TIMESTAMP,\n    success BOOLEAN,\n    exit_code INTEGER,\n    summary TEXT,\n    output_sample TEXT, -- Last 1000 lines\n    error_sample TEXT,  -- Last 1000 lines\n    FOREIGN KEY (issue_id) REFERENCES issues(id),\n    FOREIGN KEY (executor_instance_id) REFERENCES executor_instances(instance_id)\n);\n\nCREATE INDEX idx_execution_history_issue ON execution_history(issue_id);\n```\n\n## Storage Interface\n```go\n// Add to storage.Storage interface\nGetExecutionHistory(ctx context.Context, issueID string) ([]*ExecutionAttempt, error)\nRecordExecutionAttempt(ctx context.Context, attempt *ExecutionAttempt) error\n```\n\n## Integration Point\nModify internal/executor/executor.go:\n- Record attempt at start of executeIssue()\n- Update attempt on completion\n- Store truncated output/errors\n","acceptance_criteria":"\n- execution_history table created in schema\n- SQLite implementation of GetExecutionHistory\n- SQLite implementation of RecordExecutionAttempt\n- Executor records attempts at start\n- Executor updates attempts on completion\n- Output/errors truncated to 1000 lines\n- Attempt number auto-increments per issue\n- Unit tests for history storage\n- Integration test: execute same issue twice, verify history\n","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-15T01:24:29.81088-07:00","updated_at":"2025-10-15T19:48:11.193039-07:00","dependencies":[{"issue_id":"vc-99","depends_on_id":"vc-97","type":"parent-child","created_at":"2025-10-15T01:24:29.812545-07:00","created_by":"stevey"},{"issue_id":"vc-99","depends_on_id":"vc-98","type":"blocks","created_at":"2025-10-15T01:24:29.813149-07:00","created_by":"stevey"}]}
